{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('3_game_logs_standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AVG</th>\n",
       "      <th>BB%</th>\n",
       "      <th>K%</th>\n",
       "      <th>BB/K</th>\n",
       "      <th>OBP</th>\n",
       "      <th>SLG</th>\n",
       "      <th>Spd</th>\n",
       "      <th>wRAA</th>\n",
       "      <th>wFB/C</th>\n",
       "      <th>...</th>\n",
       "      <th>P_Contact%</th>\n",
       "      <th>P_Zone%</th>\n",
       "      <th>P_F-Strike%</th>\n",
       "      <th>P_SwStr%</th>\n",
       "      <th>SP SO</th>\n",
       "      <th>P_Throws_CB</th>\n",
       "      <th>P_Throws_SL</th>\n",
       "      <th>P_Throws_CH</th>\n",
       "      <th>P_Throws_CT</th>\n",
       "      <th>P_Throws_SF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31401.000000</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>3.140100e+04</td>\n",
       "      <td>31401.000000</td>\n",
       "      <td>31401.000000</td>\n",
       "      <td>31401.000000</td>\n",
       "      <td>31401.000000</td>\n",
       "      <td>31401.000000</td>\n",
       "      <td>31401.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16905.312379</td>\n",
       "      <td>3.287400e-15</td>\n",
       "      <td>3.692894e-16</td>\n",
       "      <td>-5.792775e-17</td>\n",
       "      <td>-7.240969e-18</td>\n",
       "      <td>-2.411243e-15</td>\n",
       "      <td>-2.670107e-15</td>\n",
       "      <td>2.403097e-16</td>\n",
       "      <td>-7.059945e-17</td>\n",
       "      <td>-7.150457e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.196031e-16</td>\n",
       "      <td>6.516872e-16</td>\n",
       "      <td>-4.424232e-15</td>\n",
       "      <td>3.475665e-16</td>\n",
       "      <td>4.966339</td>\n",
       "      <td>0.852043</td>\n",
       "      <td>0.769116</td>\n",
       "      <td>0.933569</td>\n",
       "      <td>0.389988</td>\n",
       "      <td>0.105984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9821.462198</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>2.581508</td>\n",
       "      <td>0.355063</td>\n",
       "      <td>0.421405</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>0.487755</td>\n",
       "      <td>0.307822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.015236e+00</td>\n",
       "      <td>-3.214047e+00</td>\n",
       "      <td>-2.962927e+00</td>\n",
       "      <td>-3.230444e+00</td>\n",
       "      <td>-4.199383e+00</td>\n",
       "      <td>-3.515029e+00</td>\n",
       "      <td>-3.272338e+00</td>\n",
       "      <td>-3.063108e+00</td>\n",
       "      <td>-3.875870e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.286863e+00</td>\n",
       "      <td>-3.797811e+00</td>\n",
       "      <td>-3.898311e+00</td>\n",
       "      <td>-2.878702e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8329.000000</td>\n",
       "      <td>-6.518455e-01</td>\n",
       "      <td>-7.106072e-01</td>\n",
       "      <td>-6.969404e-01</td>\n",
       "      <td>-7.140508e-01</td>\n",
       "      <td>-6.564628e-01</td>\n",
       "      <td>-6.529857e-01</td>\n",
       "      <td>-6.770861e-01</td>\n",
       "      <td>-7.048458e-01</td>\n",
       "      <td>-6.686061e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.776888e-01</td>\n",
       "      <td>-6.450197e-01</td>\n",
       "      <td>-6.668138e-01</td>\n",
       "      <td>-7.074731e-01</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17010.000000</td>\n",
       "      <td>6.952557e-03</td>\n",
       "      <td>-1.079847e-02</td>\n",
       "      <td>-1.155948e-02</td>\n",
       "      <td>-5.368157e-02</td>\n",
       "      <td>1.680015e-04</td>\n",
       "      <td>-2.080399e-02</td>\n",
       "      <td>1.104897e-02</td>\n",
       "      <td>-9.656734e-02</td>\n",
       "      <td>3.133481e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>9.670654e-02</td>\n",
       "      <td>2.375421e-02</td>\n",
       "      <td>6.414764e-03</td>\n",
       "      <td>-1.069205e-01</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25357.000000</td>\n",
       "      <td>6.758086e-01</td>\n",
       "      <td>7.034394e-01</td>\n",
       "      <td>6.868350e-01</td>\n",
       "      <td>6.600322e-01</td>\n",
       "      <td>6.883675e-01</td>\n",
       "      <td>6.556707e-01</td>\n",
       "      <td>6.991840e-01</td>\n",
       "      <td>6.125890e-01</td>\n",
       "      <td>6.761980e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.747597e-01</td>\n",
       "      <td>6.606817e-01</td>\n",
       "      <td>6.796434e-01</td>\n",
       "      <td>5.860247e-01</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34011.000000</td>\n",
       "      <td>3.854132e+00</td>\n",
       "      <td>4.166411e+00</td>\n",
       "      <td>4.491133e+00</td>\n",
       "      <td>5.317567e+00</td>\n",
       "      <td>3.261675e+00</td>\n",
       "      <td>3.944089e+00</td>\n",
       "      <td>4.112825e+00</td>\n",
       "      <td>4.915143e+00</td>\n",
       "      <td>5.239258e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.155571e+00</td>\n",
       "      <td>3.717934e+00</td>\n",
       "      <td>2.914762e+00</td>\n",
       "      <td>5.251856e+00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0           AVG           BB%            K%          BB/K  \\\n",
       "count  31401.000000  3.140100e+04  3.140100e+04  3.140100e+04  3.140100e+04   \n",
       "mean   16905.312379  3.287400e-15  3.692894e-16 -5.792775e-17 -7.240969e-18   \n",
       "std     9821.462198  1.000016e+00  1.000016e+00  1.000016e+00  1.000016e+00   \n",
       "min        0.000000 -4.015236e+00 -3.214047e+00 -2.962927e+00 -3.230444e+00   \n",
       "25%     8329.000000 -6.518455e-01 -7.106072e-01 -6.969404e-01 -7.140508e-01   \n",
       "50%    17010.000000  6.952557e-03 -1.079847e-02 -1.155948e-02 -5.368157e-02   \n",
       "75%    25357.000000  6.758086e-01  7.034394e-01  6.868350e-01  6.600322e-01   \n",
       "max    34011.000000  3.854132e+00  4.166411e+00  4.491133e+00  5.317567e+00   \n",
       "\n",
       "                OBP           SLG           Spd          wRAA         wFB/C  \\\n",
       "count  3.140100e+04  3.140100e+04  3.140100e+04  3.140100e+04  3.140100e+04   \n",
       "mean  -2.411243e-15 -2.670107e-15  2.403097e-16 -7.059945e-17 -7.150457e-17   \n",
       "std    1.000016e+00  1.000016e+00  1.000016e+00  1.000016e+00  1.000016e+00   \n",
       "min   -4.199383e+00 -3.515029e+00 -3.272338e+00 -3.063108e+00 -3.875870e+00   \n",
       "25%   -6.564628e-01 -6.529857e-01 -6.770861e-01 -7.048458e-01 -6.686061e-01   \n",
       "50%    1.680015e-04 -2.080399e-02  1.104897e-02 -9.656734e-02  3.133481e-03   \n",
       "75%    6.883675e-01  6.556707e-01  6.991840e-01  6.125890e-01  6.761980e-01   \n",
       "max    3.261675e+00  3.944089e+00  4.112825e+00  4.915143e+00  5.239258e+00   \n",
       "\n",
       "       ...    P_Contact%       P_Zone%   P_F-Strike%      P_SwStr%  \\\n",
       "count  ...  3.140100e+04  3.140100e+04  3.140100e+04  3.140100e+04   \n",
       "mean   ... -9.196031e-16  6.516872e-16 -4.424232e-15  3.475665e-16   \n",
       "std    ...  1.000016e+00  1.000016e+00  1.000016e+00  1.000016e+00   \n",
       "min    ... -4.286863e+00 -3.797811e+00 -3.898311e+00 -2.878702e+00   \n",
       "25%    ... -5.776888e-01 -6.450197e-01 -6.668138e-01 -7.074731e-01   \n",
       "50%    ...  9.670654e-02  2.375421e-02  6.414764e-03 -1.069205e-01   \n",
       "75%    ...  6.747597e-01  6.606817e-01  6.796434e-01  5.860247e-01   \n",
       "max    ...  3.155571e+00  3.717934e+00  2.914762e+00  5.251856e+00   \n",
       "\n",
       "              SP SO   P_Throws_CB   P_Throws_SL   P_Throws_CH   P_Throws_CT  \\\n",
       "count  31401.000000  31401.000000  31401.000000  31401.000000  31401.000000   \n",
       "mean       4.966339      0.852043      0.769116      0.933569      0.389988   \n",
       "std        2.581508      0.355063      0.421405      0.249038      0.487755   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        3.000000      1.000000      1.000000      1.000000      0.000000   \n",
       "50%        5.000000      1.000000      1.000000      1.000000      0.000000   \n",
       "75%        7.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       20.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        P_Throws_SF  \n",
       "count  31401.000000  \n",
       "mean       0.105984  \n",
       "std        0.307822  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AVG</th>\n",
       "      <th>BB%</th>\n",
       "      <th>K%</th>\n",
       "      <th>BB/K</th>\n",
       "      <th>OBP</th>\n",
       "      <th>SLG</th>\n",
       "      <th>Spd</th>\n",
       "      <th>wRAA</th>\n",
       "      <th>wFB/C</th>\n",
       "      <th>...</th>\n",
       "      <th>P_Contact%</th>\n",
       "      <th>P_Zone%</th>\n",
       "      <th>P_F-Strike%</th>\n",
       "      <th>P_SwStr%</th>\n",
       "      <th>SP SO</th>\n",
       "      <th>P_Throws_CB</th>\n",
       "      <th>P_Throws_SL</th>\n",
       "      <th>P_Throws_CH</th>\n",
       "      <th>P_Throws_CT</th>\n",
       "      <th>P_Throws_SF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.555113</td>\n",
       "      <td>-0.498981</td>\n",
       "      <td>-1.685971</td>\n",
       "      <td>0.682106</td>\n",
       "      <td>0.267150</td>\n",
       "      <td>-1.145577</td>\n",
       "      <td>-0.834374</td>\n",
       "      <td>0.809828</td>\n",
       "      <td>-0.460101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.485788</td>\n",
       "      <td>-0.047444</td>\n",
       "      <td>-0.014528</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.233257</td>\n",
       "      <td>-0.912614</td>\n",
       "      <td>-0.380277</td>\n",
       "      <td>-0.465723</td>\n",
       "      <td>-0.367834</td>\n",
       "      <td>-0.694594</td>\n",
       "      <td>0.168337</td>\n",
       "      <td>-0.439853</td>\n",
       "      <td>-0.078129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168235</td>\n",
       "      <td>1.647919</td>\n",
       "      <td>1.918384</td>\n",
       "      <td>0.447436</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.337860</td>\n",
       "      <td>0.539910</td>\n",
       "      <td>-1.039631</td>\n",
       "      <td>1.506188</td>\n",
       "      <td>0.512484</td>\n",
       "      <td>-0.397518</td>\n",
       "      <td>0.148676</td>\n",
       "      <td>0.633668</td>\n",
       "      <td>0.465976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047807</td>\n",
       "      <td>-1.441179</td>\n",
       "      <td>0.491139</td>\n",
       "      <td>-0.245510</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.812597</td>\n",
       "      <td>-0.970330</td>\n",
       "      <td>-0.575481</td>\n",
       "      <td>-0.524586</td>\n",
       "      <td>0.209424</td>\n",
       "      <td>-0.583638</td>\n",
       "      <td>-0.637764</td>\n",
       "      <td>0.608072</td>\n",
       "      <td>0.133467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120063</td>\n",
       "      <td>0.724374</td>\n",
       "      <td>-0.209018</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.351705</td>\n",
       "      <td>0.703439</td>\n",
       "      <td>-1.954918</td>\n",
       "      <td>1.800503</td>\n",
       "      <td>1.436097</td>\n",
       "      <td>1.270404</td>\n",
       "      <td>1.092404</td>\n",
       "      <td>3.208312</td>\n",
       "      <td>2.037834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794459</td>\n",
       "      <td>-0.453941</td>\n",
       "      <td>0.248777</td>\n",
       "      <td>1.001792</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31396</th>\n",
       "      <td>34007</td>\n",
       "      <td>1.247102</td>\n",
       "      <td>-0.746680</td>\n",
       "      <td>-1.962509</td>\n",
       "      <td>1.206354</td>\n",
       "      <td>0.737074</td>\n",
       "      <td>-0.161736</td>\n",
       "      <td>1.237404</td>\n",
       "      <td>0.148664</td>\n",
       "      <td>-0.232704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228727</td>\n",
       "      <td>-0.740559</td>\n",
       "      <td>-1.528546</td>\n",
       "      <td>-1.169437</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31397</th>\n",
       "      <td>34008</td>\n",
       "      <td>-0.748402</td>\n",
       "      <td>-0.287355</td>\n",
       "      <td>-1.000590</td>\n",
       "      <td>0.946989</td>\n",
       "      <td>-0.815209</td>\n",
       "      <td>-0.268666</td>\n",
       "      <td>-1.325899</td>\n",
       "      <td>-0.081511</td>\n",
       "      <td>0.051027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216406</td>\n",
       "      <td>1.106531</td>\n",
       "      <td>1.299014</td>\n",
       "      <td>0.355043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31398</th>\n",
       "      <td>34009</td>\n",
       "      <td>-1.351881</td>\n",
       "      <td>-1.047285</td>\n",
       "      <td>0.508983</td>\n",
       "      <td>-0.818900</td>\n",
       "      <td>-1.875920</td>\n",
       "      <td>-1.134840</td>\n",
       "      <td>-0.932679</td>\n",
       "      <td>-1.263438</td>\n",
       "      <td>1.004584</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.143250</td>\n",
       "      <td>0.310372</td>\n",
       "      <td>-0.209018</td>\n",
       "      <td>2.156701</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31399</th>\n",
       "      <td>34010</td>\n",
       "      <td>-1.673737</td>\n",
       "      <td>0.943923</td>\n",
       "      <td>0.292090</td>\n",
       "      <td>0.490801</td>\n",
       "      <td>-0.779130</td>\n",
       "      <td>-1.113364</td>\n",
       "      <td>-2.073017</td>\n",
       "      <td>-0.665699</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614096</td>\n",
       "      <td>0.055601</td>\n",
       "      <td>1.379801</td>\n",
       "      <td>-1.215633</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31400</th>\n",
       "      <td>34011</td>\n",
       "      <td>-0.482871</td>\n",
       "      <td>-1.499395</td>\n",
       "      <td>1.285459</td>\n",
       "      <td>-1.598835</td>\n",
       "      <td>-1.313094</td>\n",
       "      <td>-0.980933</td>\n",
       "      <td>1.760878</td>\n",
       "      <td>-0.567833</td>\n",
       "      <td>-1.454879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240491</td>\n",
       "      <td>0.405911</td>\n",
       "      <td>1.191297</td>\n",
       "      <td>0.493632</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31401 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       AVG       BB%        K%      BB/K       OBP       SLG  \\\n",
       "0               0  0.555113 -0.498981 -1.685971  0.682106  0.267150 -1.145577   \n",
       "1               1  0.233257 -0.912614 -0.380277 -0.465723 -0.367834 -0.694594   \n",
       "2               2  0.337860  0.539910 -1.039631  1.506188  0.512484 -0.397518   \n",
       "3               3  0.812597 -0.970330 -0.575481 -0.524586  0.209424 -0.583638   \n",
       "4               4  1.351705  0.703439 -1.954918  1.800503  1.436097  1.270404   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "31396       34007  1.247102 -0.746680 -1.962509  1.206354  0.737074 -0.161736   \n",
       "31397       34008 -0.748402 -0.287355 -1.000590  0.946989 -0.815209 -0.268666   \n",
       "31398       34009 -1.351881 -1.047285  0.508983 -0.818900 -1.875920 -1.134840   \n",
       "31399       34010 -1.673737  0.943923  0.292090  0.490801 -0.779130 -1.113364   \n",
       "31400       34011 -0.482871 -1.499395  1.285459 -1.598835 -1.313094 -0.980933   \n",
       "\n",
       "            Spd      wRAA     wFB/C  ...  P_Contact%   P_Zone%  P_F-Strike%  \\\n",
       "0     -0.834374  0.809828 -0.460101  ...   -0.071892 -0.485788    -0.047444   \n",
       "1      0.168337 -0.439853 -0.078129  ...   -0.168235  1.647919     1.918384   \n",
       "2      0.148676  0.633668  0.465976  ...   -0.047807 -1.441179     0.491139   \n",
       "3     -0.637764  0.608072  0.133467  ...   -0.120063  0.724374    -0.209018   \n",
       "4      1.092404  3.208312  2.037834  ...   -0.794459 -0.453941     0.248777   \n",
       "...         ...       ...       ...  ...         ...       ...          ...   \n",
       "31396  1.237404  0.148664 -0.232704  ...    1.228727 -0.740559    -1.528546   \n",
       "31397 -1.325899 -0.081511  0.051027  ...   -0.216406  1.106531     1.299014   \n",
       "31398 -0.932679 -1.263438  1.004584  ...   -2.143250  0.310372    -0.209018   \n",
       "31399 -2.073017 -0.665699  1.749293  ...    1.614096  0.055601     1.379801   \n",
       "31400  1.760878 -0.567833 -1.454879  ...   -0.240491  0.405911     1.191297   \n",
       "\n",
       "       P_SwStr%  SP SO  P_Throws_CB  P_Throws_SL  P_Throws_CH  P_Throws_CT  \\\n",
       "0     -0.014528    6.0            1            0            1            1   \n",
       "1      0.447436    5.0            1            0            1            1   \n",
       "2     -0.245510    4.0            0            1            1            1   \n",
       "3      0.031669    2.0            1            0            1            1   \n",
       "4      1.001792    6.0            1            1            1            1   \n",
       "...         ...    ...          ...          ...          ...          ...   \n",
       "31396 -1.169437    4.0            1            1            1            0   \n",
       "31397  0.355043    0.0            1            0            1            1   \n",
       "31398  2.156701    6.0            1            1            0            1   \n",
       "31399 -1.215633    5.0            0            1            1            1   \n",
       "31400  0.493632    6.0            1            0            1            0   \n",
       "\n",
       "       P_Throws_SF  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                1  \n",
       "...            ...  \n",
       "31396            0  \n",
       "31397            0  \n",
       "31398            1  \n",
       "31399            0  \n",
       "31400            0  \n",
       "\n",
       "[31401 rows x 63 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SP SO', axis=1)\n",
    "y = df['SP SO']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-2.32558105e-05 -7.48291253e+00  6.51645714e+00  1.43070794e+01\n",
      " -1.76145323e+00  4.47419095e+00  1.24943056e+00  4.25334879e-02\n",
      " -7.01341020e-03  1.34093361e-01  1.00057324e-02 -1.38417296e-02\n",
      "  4.03403800e-02 -1.47973684e-02 -6.74273072e-03 -4.40090479e+01\n",
      " -3.16904786e+01  7.54919958e+01  8.56935854e+00  1.68749599e+01\n",
      " -1.94731382e+01 -7.53709339e+00 -4.40168692e+00  1.19713520e+01\n",
      " -7.42504569e-02  5.46790526e-01  2.14220053e-02  6.71643738e+00\n",
      " -2.23940564e+00 -6.83206863e-02 -8.61185190e-01 -5.56975876e-03\n",
      " -3.13042241e-01  2.67796179e-03 -4.80487877e-01  1.71751240e-02\n",
      " -3.31104485e-01  1.19060728e-02 -5.08209592e-01  8.66012089e-03\n",
      "  2.64639847e-01 -3.34639039e-02 -3.40277916e-03  7.50484212e-03\n",
      "  1.35321408e-02 -4.65052547e-03  3.87677633e-03  1.63209495e-02\n",
      "  1.02059334e+01  4.06104462e+00 -2.04299902e+01 -2.79071193e+00\n",
      " -3.31763447e+00  2.22382245e+01  4.22599280e+00 -4.58855004e-01\n",
      "  3.47705241e+01  1.50309007e-01 -1.52871184e-02  5.33161069e-01\n",
      "  4.09243182e-02  9.28795549e-02]\n",
      "Mean squared error: 5.10\n",
      "Coefficient of determination: 0.23\n",
      "Mean Absolute Error for Linear Regression: 1.799909272993893\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Coefficients: \\n', model.coef_)\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "print(f\"Mean Absolute Error for Linear Regression: {mae_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 5.33\n",
      "Coefficient of determination: 0.19\n",
      "Mean absolute error: 1.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 20, random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Mean squared error: %.2f' % mse)\n",
    "print('Coefficient of determination: %.2f' % r2)\n",
    "print('Mean absolute error: %.2f' % mae_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 5.15\n",
      "Coefficient of determination: 0.22\n",
      "Mean absolute error: 1.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Mean squared error: %.2f' % mse)\n",
    "print('Coefficient of determination: %.2f' % r2)\n",
    "print('Mean absolute error: %.2f' % mae_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 10.53\n",
      "Coefficient of determination: -0.60\n",
      "Mean absolute error: 2.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Mean squared error: %.2f' % mse)\n",
    "print('Coefficient of determination: %.2f' % r2)\n",
    "print('Mean absolute error: %.2f' % mae_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 1074.4237 - val_loss: 137.6986\n",
      "Epoch 2/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 16.3651 - val_loss: 8.9316\n",
      "Epoch 3/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 20.1338 - val_loss: 24.0949\n",
      "Epoch 4/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 34.5978 - val_loss: 17.4085\n",
      "Epoch 5/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 28.2985 - val_loss: 6.7524\n",
      "Epoch 6/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 38.4227 - val_loss: 19.8417\n",
      "Epoch 7/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 20.7826 - val_loss: 11.7268\n",
      "Epoch 8/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 22.6862 - val_loss: 139.9636\n",
      "Epoch 9/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 22.1780 - val_loss: 7.1390\n",
      "Epoch 10/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 16.3698 - val_loss: 34.9210\n",
      "Epoch 11/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 12.7776 - val_loss: 8.9275\n",
      "Epoch 12/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 15.9318 - val_loss: 10.2974\n",
      "Epoch 13/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 20.5148 - val_loss: 5.9298\n",
      "Epoch 14/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 11.9663 - val_loss: 25.1183\n",
      "Epoch 15/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 9.8276 - val_loss: 6.5815\n",
      "Epoch 16/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 11.0344 - val_loss: 10.4211\n",
      "Epoch 17/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 12.2309 - val_loss: 12.5402\n",
      "Epoch 18/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 11.3226 - val_loss: 10.8097\n",
      "Epoch 19/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 9.3815 - val_loss: 13.0400\n",
      "Epoch 20/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 8.7241 - val_loss: 13.7788\n",
      "Epoch 21/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 8.5275 - val_loss: 5.7690\n",
      "Epoch 22/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 7.6586 - val_loss: 22.9732\n",
      "Epoch 23/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 8.7582 - val_loss: 7.2896\n",
      "Epoch 24/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 7.1220 - val_loss: 5.3556\n",
      "Epoch 25/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 6.7686 - val_loss: 5.3301\n",
      "Epoch 26/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 6.1876 - val_loss: 12.6675\n",
      "Epoch 27/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 6.1984 - val_loss: 5.4212\n",
      "Epoch 28/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 6.0634 - val_loss: 5.9601\n",
      "Epoch 29/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.9727 - val_loss: 5.1381\n",
      "Epoch 30/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7818 - val_loss: 6.8702\n",
      "Epoch 31/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.9207 - val_loss: 5.4704\n",
      "Epoch 32/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.6653 - val_loss: 5.4707\n",
      "Epoch 33/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7454 - val_loss: 5.2009\n",
      "Epoch 34/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7203 - val_loss: 5.0757\n",
      "Epoch 35/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7665 - val_loss: 5.1909\n",
      "Epoch 36/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.5519 - val_loss: 5.8459\n",
      "Epoch 37/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.5039 - val_loss: 5.1039\n",
      "Epoch 38/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.5329 - val_loss: 5.6118\n",
      "Epoch 39/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4766 - val_loss: 5.3519\n",
      "Epoch 40/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7377 - val_loss: 5.4800\n",
      "Epoch 41/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4818 - val_loss: 5.8317\n",
      "Epoch 42/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.7092 - val_loss: 5.3613\n",
      "Epoch 43/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.6683 - val_loss: 5.2613\n",
      "Epoch 44/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.5319 - val_loss: 6.1942\n",
      "Epoch 45/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4357 - val_loss: 5.1693\n",
      "Epoch 46/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4569 - val_loss: 5.7602\n",
      "Epoch 47/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4716 - val_loss: 5.4440\n",
      "Epoch 48/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4556 - val_loss: 5.2141\n",
      "Epoch 49/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4750 - val_loss: 5.1551\n",
      "Epoch 50/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3873 - val_loss: 5.4138\n",
      "Epoch 51/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4103 - val_loss: 5.1929\n",
      "Epoch 52/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3543 - val_loss: 5.1089\n",
      "Epoch 53/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3457 - val_loss: 5.3181\n",
      "Epoch 54/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4063 - val_loss: 5.7968\n",
      "Epoch 55/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3797 - val_loss: 5.3453\n",
      "Epoch 56/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4531 - val_loss: 5.6521\n",
      "Epoch 57/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.4333 - val_loss: 5.1188\n",
      "Epoch 58/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3958 - val_loss: 5.2935\n",
      "Epoch 59/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2894 - val_loss: 5.2023\n",
      "Epoch 60/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3492 - val_loss: 5.1725\n",
      "Epoch 61/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3046 - val_loss: 5.0916\n",
      "Epoch 62/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3351 - val_loss: 5.0866\n",
      "Epoch 63/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3044 - val_loss: 5.2526\n",
      "Epoch 64/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3170 - val_loss: 5.1501\n",
      "Epoch 65/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2708 - val_loss: 5.4108\n",
      "Epoch 66/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2753 - val_loss: 5.1433\n",
      "Epoch 67/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2549 - val_loss: 5.8854\n",
      "Epoch 68/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3697 - val_loss: 5.1441\n",
      "Epoch 69/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2841 - val_loss: 5.3719\n",
      "Epoch 70/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3006 - val_loss: 5.1353\n",
      "Epoch 71/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2393 - val_loss: 5.3592\n",
      "Epoch 72/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2554 - val_loss: 5.3747\n",
      "Epoch 73/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2086 - val_loss: 5.3148\n",
      "Epoch 74/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2110 - val_loss: 5.1612\n",
      "Epoch 75/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2523 - val_loss: 5.0226\n",
      "Epoch 76/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1877 - val_loss: 5.0368\n",
      "Epoch 77/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.3041 - val_loss: 5.1727\n",
      "Epoch 78/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2184 - val_loss: 5.1066\n",
      "Epoch 79/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1865 - val_loss: 5.0392\n",
      "Epoch 80/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1893 - val_loss: 5.0402\n",
      "Epoch 81/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1857 - val_loss: 5.1235\n",
      "Epoch 82/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1737 - val_loss: 5.0399\n",
      "Epoch 83/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1889 - val_loss: 5.0905\n",
      "Epoch 84/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1976 - val_loss: 5.0837\n",
      "Epoch 85/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.2221 - val_loss: 5.0340\n",
      "Epoch 86/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1911 - val_loss: 5.0016\n",
      "Epoch 87/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1537 - val_loss: 5.0433\n",
      "Epoch 88/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1794 - val_loss: 5.1952\n",
      "Epoch 89/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1513 - val_loss: 5.3896\n",
      "Epoch 90/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1831 - val_loss: 5.1430\n",
      "Epoch 91/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1537 - val_loss: 5.0784\n",
      "Epoch 92/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1980 - val_loss: 5.2367\n",
      "Epoch 93/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1323 - val_loss: 5.1297\n",
      "Epoch 94/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1408 - val_loss: 5.0358\n",
      "Epoch 95/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1179 - val_loss: 5.1578\n",
      "Epoch 96/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1597 - val_loss: 5.3241\n",
      "Epoch 97/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1664 - val_loss: 5.1160\n",
      "Epoch 98/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1214 - val_loss: 6.1175\n",
      "Epoch 99/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1995 - val_loss: 5.0907\n",
      "Epoch 100/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1374 - val_loss: 4.9999\n",
      "Epoch 101/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1277 - val_loss: 5.0345\n",
      "Epoch 102/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1421 - val_loss: 5.3292\n",
      "Epoch 103/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1183 - val_loss: 5.3618\n",
      "Epoch 104/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1265 - val_loss: 5.0662\n",
      "Epoch 105/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1351 - val_loss: 5.1792\n",
      "Epoch 106/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1169 - val_loss: 5.0647\n",
      "Epoch 107/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1819 - val_loss: 5.0798\n",
      "Epoch 108/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1082 - val_loss: 5.0327\n",
      "Epoch 109/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0982 - val_loss: 5.0449\n",
      "Epoch 110/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1071 - val_loss: 5.0704\n",
      "Epoch 111/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1299 - val_loss: 5.0352\n",
      "Epoch 112/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1131 - val_loss: 5.0846\n",
      "Epoch 113/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1058 - val_loss: 5.6217\n",
      "Epoch 114/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1398 - val_loss: 5.1025\n",
      "Epoch 115/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1286 - val_loss: 5.2749\n",
      "Epoch 116/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1041 - val_loss: 5.2839\n",
      "Epoch 117/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1197 - val_loss: 5.0792\n",
      "Epoch 118/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1226 - val_loss: 5.0704\n",
      "Epoch 119/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1019 - val_loss: 5.2046\n",
      "Epoch 120/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1426 - val_loss: 5.0418\n",
      "Epoch 121/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1141 - val_loss: 5.2583\n",
      "Epoch 122/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1118 - val_loss: 5.1148\n",
      "Epoch 123/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1034 - val_loss: 5.1099\n",
      "Epoch 124/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1005 - val_loss: 5.1838\n",
      "Epoch 125/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0878 - val_loss: 5.0415\n",
      "Epoch 126/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1056 - val_loss: 5.0512\n",
      "Epoch 127/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0953 - val_loss: 5.0122\n",
      "Epoch 128/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1054 - val_loss: 5.1165\n",
      "Epoch 129/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0906 - val_loss: 5.0293\n",
      "Epoch 130/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0743 - val_loss: 5.0234\n",
      "Epoch 131/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1114 - val_loss: 5.2810\n",
      "Epoch 132/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0818 - val_loss: 5.1838\n",
      "Epoch 133/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0700 - val_loss: 5.1165\n",
      "Epoch 134/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1182 - val_loss: 5.0490\n",
      "Epoch 135/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0828 - val_loss: 5.1148\n",
      "Epoch 136/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1038 - val_loss: 5.1509\n",
      "Epoch 137/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0923 - val_loss: 5.0889\n",
      "Epoch 138/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1122 - val_loss: 5.0352\n",
      "Epoch 139/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0874 - val_loss: 5.1669\n",
      "Epoch 140/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1094 - val_loss: 5.3916\n",
      "Epoch 141/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1645 - val_loss: 5.0182\n",
      "Epoch 142/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0626 - val_loss: 5.0701\n",
      "Epoch 143/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0876 - val_loss: 5.0970\n",
      "Epoch 144/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0947 - val_loss: 5.1230\n",
      "Epoch 145/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0670 - val_loss: 5.0865\n",
      "Epoch 146/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0921 - val_loss: 5.0509\n",
      "Epoch 147/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0884 - val_loss: 5.1106\n",
      "Epoch 148/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0810 - val_loss: 5.0766\n",
      "Epoch 149/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0614 - val_loss: 5.0966\n",
      "Epoch 150/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0599 - val_loss: 5.0742\n",
      "Epoch 151/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0799 - val_loss: 5.0618\n",
      "Epoch 152/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0852 - val_loss: 5.0481\n",
      "Epoch 153/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0601 - val_loss: 5.2087\n",
      "Epoch 154/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0661 - val_loss: 5.0703\n",
      "Epoch 155/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0955 - val_loss: 5.1256\n",
      "Epoch 156/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0634 - val_loss: 5.0656\n",
      "Epoch 157/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0615 - val_loss: 5.1159\n",
      "Epoch 158/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1106 - val_loss: 5.1875\n",
      "Epoch 159/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0815 - val_loss: 5.0901\n",
      "Epoch 160/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0714 - val_loss: 5.1177\n",
      "Epoch 161/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0432 - val_loss: 5.1731\n",
      "Epoch 162/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0688 - val_loss: 5.0608\n",
      "Epoch 163/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0781 - val_loss: 5.1559\n",
      "Epoch 164/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0555 - val_loss: 5.1774\n",
      "Epoch 165/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0808 - val_loss: 5.0160\n",
      "Epoch 166/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0670 - val_loss: 5.1033\n",
      "Epoch 167/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0697 - val_loss: 5.1602\n",
      "Epoch 168/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0741 - val_loss: 5.1286\n",
      "Epoch 169/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0532 - val_loss: 5.0628\n",
      "Epoch 170/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0788 - val_loss: 5.0386\n",
      "Epoch 171/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0898 - val_loss: 5.1531\n",
      "Epoch 172/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1256 - val_loss: 5.0858\n",
      "Epoch 173/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1103 - val_loss: 5.2307\n",
      "Epoch 174/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0782 - val_loss: 5.3121\n",
      "Epoch 175/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0584 - val_loss: 5.2930\n",
      "Epoch 176/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0452 - val_loss: 5.0363\n",
      "Epoch 177/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0343 - val_loss: 5.1979\n",
      "Epoch 178/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0588 - val_loss: 5.1205\n",
      "Epoch 179/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0412 - val_loss: 5.0777\n",
      "Epoch 180/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0474 - val_loss: 5.0467\n",
      "Epoch 181/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0381 - val_loss: 5.1105\n",
      "Epoch 182/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1332 - val_loss: 5.0282\n",
      "Epoch 183/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0446 - val_loss: 5.1815\n",
      "Epoch 184/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0355 - val_loss: 5.1147\n",
      "Epoch 185/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0413 - val_loss: 5.0501\n",
      "Epoch 186/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0566 - val_loss: 5.1033\n",
      "Epoch 187/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0250 - val_loss: 5.1225\n",
      "Epoch 188/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0390 - val_loss: 5.1649\n",
      "Epoch 189/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0433 - val_loss: 5.0537\n",
      "Epoch 190/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0407 - val_loss: 5.0922\n",
      "Epoch 191/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0597 - val_loss: 5.1362\n",
      "Epoch 192/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0455 - val_loss: 5.0452\n",
      "Epoch 193/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0364 - val_loss: 5.1773\n",
      "Epoch 194/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0413 - val_loss: 5.0826\n",
      "Epoch 195/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0342 - val_loss: 5.2451\n",
      "Epoch 196/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0428 - val_loss: 5.2394\n",
      "Epoch 197/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0232 - val_loss: 5.0594\n",
      "Epoch 198/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0339 - val_loss: 5.0824\n",
      "Epoch 199/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0272 - val_loss: 5.2131\n",
      "Epoch 200/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0358 - val_loss: 5.0680\n",
      "Epoch 201/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0524 - val_loss: 5.1168\n",
      "Epoch 202/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0239 - val_loss: 5.1637\n",
      "Epoch 203/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0060 - val_loss: 5.1816\n",
      "Epoch 204/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0229 - val_loss: 5.1169\n",
      "Epoch 205/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0411 - val_loss: 5.0609\n",
      "Epoch 206/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0214 - val_loss: 5.4071\n",
      "Epoch 207/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0343 - val_loss: 5.0892\n",
      "Epoch 208/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0095 - val_loss: 5.1026\n",
      "Epoch 209/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0109 - val_loss: 5.0970\n",
      "Epoch 210/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0124 - val_loss: 5.1321\n",
      "Epoch 211/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0397 - val_loss: 5.1296\n",
      "Epoch 212/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0579 - val_loss: 5.1177\n",
      "Epoch 213/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0176 - val_loss: 5.2179\n",
      "Epoch 214/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0110 - val_loss: 5.3486\n",
      "Epoch 215/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0267 - val_loss: 5.1409\n",
      "Epoch 216/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0400 - val_loss: 5.2772\n",
      "Epoch 217/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0407 - val_loss: 5.1894\n",
      "Epoch 218/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0279 - val_loss: 5.0535\n",
      "Epoch 219/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0099 - val_loss: 5.1989\n",
      "Epoch 220/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9950 - val_loss: 5.0628\n",
      "Epoch 221/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0048 - val_loss: 5.2249\n",
      "Epoch 222/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0051 - val_loss: 5.1822\n",
      "Epoch 223/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0184 - val_loss: 5.2646\n",
      "Epoch 224/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0092 - val_loss: 5.3943\n",
      "Epoch 225/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0162 - val_loss: 5.1077\n",
      "Epoch 226/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0353 - val_loss: 5.0809\n",
      "Epoch 227/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0008 - val_loss: 5.2115\n",
      "Epoch 228/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9997 - val_loss: 5.1304\n",
      "Epoch 229/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9854 - val_loss: 5.2075\n",
      "Epoch 230/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0077 - val_loss: 5.0901\n",
      "Epoch 231/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0065 - val_loss: 5.1895\n",
      "Epoch 232/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0053 - val_loss: 5.1612\n",
      "Epoch 233/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0145 - val_loss: 5.3994\n",
      "Epoch 234/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0223 - val_loss: 5.1422\n",
      "Epoch 235/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9887 - val_loss: 5.1322\n",
      "Epoch 236/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9974 - val_loss: 5.1114\n",
      "Epoch 237/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0025 - val_loss: 5.1176\n",
      "Epoch 238/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 5.0072 - val_loss: 5.0908\n",
      "Epoch 239/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0082 - val_loss: 5.1780\n",
      "Epoch 240/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0013 - val_loss: 5.2320\n",
      "Epoch 241/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0052 - val_loss: 5.1497\n",
      "Epoch 242/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9760 - val_loss: 5.3340\n",
      "Epoch 243/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0179 - val_loss: 5.0866\n",
      "Epoch 244/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0007 - val_loss: 5.1397\n",
      "Epoch 245/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9969 - val_loss: 5.0971\n",
      "Epoch 246/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9819 - val_loss: 5.1441\n",
      "Epoch 247/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0009 - val_loss: 5.1340\n",
      "Epoch 248/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9843 - val_loss: 5.0952\n",
      "Epoch 249/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9950 - val_loss: 5.1243\n",
      "Epoch 250/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9992 - val_loss: 5.1410\n",
      "Epoch 251/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9864 - val_loss: 5.2000\n",
      "Epoch 252/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9875 - val_loss: 5.1598\n",
      "Epoch 253/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9969 - val_loss: 5.1240\n",
      "Epoch 254/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0274 - val_loss: 5.1800\n",
      "Epoch 255/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9978 - val_loss: 5.0666\n",
      "Epoch 256/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9920 - val_loss: 5.1566\n",
      "Epoch 257/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9741 - val_loss: 5.1184\n",
      "Epoch 258/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0042 - val_loss: 5.1002\n",
      "Epoch 259/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0020 - val_loss: 5.1176\n",
      "Epoch 260/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9701 - val_loss: 5.1464\n",
      "Epoch 261/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9966 - val_loss: 5.1459\n",
      "Epoch 262/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9919 - val_loss: 5.1161\n",
      "Epoch 263/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9789 - val_loss: 5.0958\n",
      "Epoch 264/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0008 - val_loss: 5.4174\n",
      "Epoch 265/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9847 - val_loss: 5.0773\n",
      "Epoch 266/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9857 - val_loss: 5.0761\n",
      "Epoch 267/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0022 - val_loss: 5.2067\n",
      "Epoch 268/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9891 - val_loss: 5.0659\n",
      "Epoch 269/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9873 - val_loss: 5.2804\n",
      "Epoch 270/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0074 - val_loss: 6.2065\n",
      "Epoch 271/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0650 - val_loss: 5.1499\n",
      "Epoch 272/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9891 - val_loss: 5.1475\n",
      "Epoch 273/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9880 - val_loss: 5.2841\n",
      "Epoch 274/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0019 - val_loss: 5.0809\n",
      "Epoch 275/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9892 - val_loss: 5.0739\n",
      "Epoch 276/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9878 - val_loss: 5.0927\n",
      "Epoch 277/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9712 - val_loss: 5.1120\n",
      "Epoch 278/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9846 - val_loss: 5.0902\n",
      "Epoch 279/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9631 - val_loss: 5.4079\n",
      "Epoch 280/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9834 - val_loss: 5.2207\n",
      "Epoch 281/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9799 - val_loss: 5.1548\n",
      "Epoch 282/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9663 - val_loss: 5.1100\n",
      "Epoch 283/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9890 - val_loss: 5.6527\n",
      "Epoch 284/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0038 - val_loss: 5.3004\n",
      "Epoch 285/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9789 - val_loss: 5.1964\n",
      "Epoch 286/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9763 - val_loss: 5.2298\n",
      "Epoch 287/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9722 - val_loss: 5.0990\n",
      "Epoch 288/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9711 - val_loss: 5.2725\n",
      "Epoch 289/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9649 - val_loss: 5.2903\n",
      "Epoch 290/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9677 - val_loss: 5.1739\n",
      "Epoch 291/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9723 - val_loss: 5.1022\n",
      "Epoch 292/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9734 - val_loss: 5.2771\n",
      "Epoch 293/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9694 - val_loss: 5.1563\n",
      "Epoch 294/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0281 - val_loss: 5.1554\n",
      "Epoch 295/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0815 - val_loss: 5.0661\n",
      "Epoch 296/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9508 - val_loss: 5.2858\n",
      "Epoch 297/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9552 - val_loss: 5.1023\n",
      "Epoch 298/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9690 - val_loss: 5.0882\n",
      "Epoch 299/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9734 - val_loss: 5.0891\n",
      "Epoch 300/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9591 - val_loss: 5.2206\n",
      "Epoch 301/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9610 - val_loss: 5.2560\n",
      "Epoch 302/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9582 - val_loss: 5.1085\n",
      "Epoch 303/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9621 - val_loss: 5.0611\n",
      "Epoch 304/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9861 - val_loss: 5.1220\n",
      "Epoch 305/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9652 - val_loss: 5.0697\n",
      "Epoch 306/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9474 - val_loss: 5.0625\n",
      "Epoch 307/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9699 - val_loss: 5.1250\n",
      "Epoch 308/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9866 - val_loss: 5.3541\n",
      "Epoch 309/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9727 - val_loss: 5.1199\n",
      "Epoch 310/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9854 - val_loss: 5.0707\n",
      "Epoch 311/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9555 - val_loss: 5.1713\n",
      "Epoch 312/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9775 - val_loss: 5.0888\n",
      "Epoch 313/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9535 - val_loss: 5.3800\n",
      "Epoch 314/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9629 - val_loss: 5.2507\n",
      "Epoch 315/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9828 - val_loss: 5.1071\n",
      "Epoch 316/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0388 - val_loss: 5.0880\n",
      "Epoch 317/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0884 - val_loss: 5.3207\n",
      "Epoch 318/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0568 - val_loss: 5.1582\n",
      "Epoch 319/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9868 - val_loss: 5.1054\n",
      "Epoch 320/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9826 - val_loss: 5.2305\n",
      "Epoch 321/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9627 - val_loss: 5.2996\n",
      "Epoch 322/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9770 - val_loss: 5.1262\n",
      "Epoch 323/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0467 - val_loss: 5.2212\n",
      "Epoch 324/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0027 - val_loss: 5.1256\n",
      "Epoch 325/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0387 - val_loss: 5.1558\n",
      "Epoch 326/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9985 - val_loss: 5.1101\n",
      "Epoch 327/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0349 - val_loss: 5.4127\n",
      "Epoch 328/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0120 - val_loss: 5.0970\n",
      "Epoch 329/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9898 - val_loss: 5.1145\n",
      "Epoch 330/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0072 - val_loss: 5.2637\n",
      "Epoch 331/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0123 - val_loss: 5.3555\n",
      "Epoch 332/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9880 - val_loss: 5.1472\n",
      "Epoch 333/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0003 - val_loss: 5.0697\n",
      "Epoch 334/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9786 - val_loss: 5.1723\n",
      "Epoch 335/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9882 - val_loss: 5.0827\n",
      "Epoch 336/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0080 - val_loss: 5.1586\n",
      "Epoch 337/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9996 - val_loss: 5.0587\n",
      "Epoch 338/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0235 - val_loss: 5.1445\n",
      "Epoch 339/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0211 - val_loss: 5.1122\n",
      "Epoch 340/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0252 - val_loss: 5.0825\n",
      "Epoch 341/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0216 - val_loss: 5.0611\n",
      "Epoch 342/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9646 - val_loss: 5.1070\n",
      "Epoch 343/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9429 - val_loss: 5.0757\n",
      "Epoch 344/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9527 - val_loss: 5.0977\n",
      "Epoch 345/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9336 - val_loss: 5.0762\n",
      "Epoch 346/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9476 - val_loss: 5.0888\n",
      "Epoch 347/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9401 - val_loss: 5.1174\n",
      "Epoch 348/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9390 - val_loss: 5.1135\n",
      "Epoch 349/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9551 - val_loss: 5.1270\n",
      "Epoch 350/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9324 - val_loss: 5.0778\n",
      "Epoch 351/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9444 - val_loss: 5.1192\n",
      "Epoch 352/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9588 - val_loss: 5.0608\n",
      "Epoch 353/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9624 - val_loss: 5.1223\n",
      "Epoch 354/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9868 - val_loss: 5.1821\n",
      "Epoch 355/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9436 - val_loss: 5.2565\n",
      "Epoch 356/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9257 - val_loss: 5.1545\n",
      "Epoch 357/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9455 - val_loss: 5.2601\n",
      "Epoch 358/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9538 - val_loss: 5.1709\n",
      "Epoch 359/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9338 - val_loss: 5.1119\n",
      "Epoch 360/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9326 - val_loss: 5.1241\n",
      "Epoch 361/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9985 - val_loss: 5.1632\n",
      "Epoch 362/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.1110 - val_loss: 5.1239\n",
      "Epoch 363/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0300 - val_loss: 5.4302\n",
      "Epoch 364/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9733 - val_loss: 5.2024\n",
      "Epoch 365/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9499 - val_loss: 5.0675\n",
      "Epoch 366/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9893 - val_loss: 5.0336\n",
      "Epoch 367/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0035 - val_loss: 5.2863\n",
      "Epoch 368/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9748 - val_loss: 5.1316\n",
      "Epoch 369/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9695 - val_loss: 5.2158\n",
      "Epoch 370/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9795 - val_loss: 5.0993\n",
      "Epoch 371/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9332 - val_loss: 5.3393\n",
      "Epoch 372/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9790 - val_loss: 5.1241\n",
      "Epoch 373/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9919 - val_loss: 5.2447\n",
      "Epoch 374/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0358 - val_loss: 5.1196\n",
      "Epoch 375/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9541 - val_loss: 5.0895\n",
      "Epoch 376/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9608 - val_loss: 5.1213\n",
      "Epoch 377/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9320 - val_loss: 5.2529\n",
      "Epoch 378/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0312 - val_loss: 5.1886\n",
      "Epoch 379/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9668 - val_loss: 5.0509\n",
      "Epoch 380/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9398 - val_loss: 5.1148\n",
      "Epoch 381/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9499 - val_loss: 5.1170\n",
      "Epoch 382/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9651 - val_loss: 5.0957\n",
      "Epoch 383/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9775 - val_loss: 5.1661\n",
      "Epoch 384/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0398 - val_loss: 5.1124\n",
      "Epoch 385/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9305 - val_loss: 5.2497\n",
      "Epoch 386/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0128 - val_loss: 5.2180\n",
      "Epoch 387/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9727 - val_loss: 5.3555\n",
      "Epoch 388/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9810 - val_loss: 5.1703\n",
      "Epoch 389/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9404 - val_loss: 5.1266\n",
      "Epoch 390/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9542 - val_loss: 5.0964\n",
      "Epoch 391/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9321 - val_loss: 5.0803\n",
      "Epoch 392/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9313 - val_loss: 5.1498\n",
      "Epoch 393/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9272 - val_loss: 5.0756\n",
      "Epoch 394/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9340 - val_loss: 5.1361\n",
      "Epoch 395/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9380 - val_loss: 5.0960\n",
      "Epoch 396/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9272 - val_loss: 5.3251\n",
      "Epoch 397/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9343 - val_loss: 5.1523\n",
      "Epoch 398/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9268 - val_loss: 5.1614\n",
      "Epoch 399/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9252 - val_loss: 5.1052\n",
      "Epoch 400/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9580 - val_loss: 5.1108\n",
      "Epoch 401/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9926 - val_loss: 5.1101\n",
      "Epoch 402/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0145 - val_loss: 5.0854\n",
      "Epoch 403/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0308 - val_loss: 5.1706\n",
      "Epoch 404/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9627 - val_loss: 5.0992\n",
      "Epoch 405/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9633 - val_loss: 5.2145\n",
      "Epoch 406/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0035 - val_loss: 5.1372\n",
      "Epoch 407/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9769 - val_loss: 5.2103\n",
      "Epoch 408/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9804 - val_loss: 5.1438\n",
      "Epoch 409/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9349 - val_loss: 5.0731\n",
      "Epoch 410/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8980 - val_loss: 5.2990\n",
      "Epoch 411/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9164 - val_loss: 5.1534\n",
      "Epoch 412/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9597 - val_loss: 5.0491\n",
      "Epoch 413/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9106 - val_loss: 5.0856\n",
      "Epoch 414/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9309 - val_loss: 5.2981\n",
      "Epoch 415/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9448 - val_loss: 5.1370\n",
      "Epoch 416/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8975 - val_loss: 5.0872\n",
      "Epoch 417/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9165 - val_loss: 5.2324\n",
      "Epoch 418/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9877 - val_loss: 5.1826\n",
      "Epoch 419/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9691 - val_loss: 5.4879\n",
      "Epoch 420/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0142 - val_loss: 5.3069\n",
      "Epoch 421/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9416 - val_loss: 5.1715\n",
      "Epoch 422/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9545 - val_loss: 5.1116\n",
      "Epoch 423/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9666 - val_loss: 5.1099\n",
      "Epoch 424/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9690 - val_loss: 5.2837\n",
      "Epoch 425/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0083 - val_loss: 5.0934\n",
      "Epoch 426/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9340 - val_loss: 5.0600\n",
      "Epoch 427/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9402 - val_loss: 5.1803\n",
      "Epoch 428/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9909 - val_loss: 5.1310\n",
      "Epoch 429/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9079 - val_loss: 5.1927\n",
      "Epoch 430/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9418 - val_loss: 5.1687\n",
      "Epoch 431/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9631 - val_loss: 5.0787\n",
      "Epoch 432/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9267 - val_loss: 5.1208\n",
      "Epoch 433/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9347 - val_loss: 5.3473\n",
      "Epoch 434/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0221 - val_loss: 5.0621\n",
      "Epoch 435/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9433 - val_loss: 5.3511\n",
      "Epoch 436/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9362 - val_loss: 5.2242\n",
      "Epoch 437/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9249 - val_loss: 5.1675\n",
      "Epoch 438/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9193 - val_loss: 5.2486\n",
      "Epoch 439/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9169 - val_loss: 5.0910\n",
      "Epoch 440/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9752 - val_loss: 5.1287\n",
      "Epoch 441/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9191 - val_loss: 5.1060\n",
      "Epoch 442/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0299 - val_loss: 5.0869\n",
      "Epoch 443/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9669 - val_loss: 5.1398\n",
      "Epoch 444/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9621 - val_loss: 5.1939\n",
      "Epoch 445/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9211 - val_loss: 5.0980\n",
      "Epoch 446/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9168 - val_loss: 5.1860\n",
      "Epoch 447/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9910 - val_loss: 5.1093\n",
      "Epoch 448/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9583 - val_loss: 5.0910\n",
      "Epoch 449/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9673 - val_loss: 5.1247\n",
      "Epoch 450/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9828 - val_loss: 5.0375\n",
      "Epoch 451/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9314 - val_loss: 5.0504\n",
      "Epoch 452/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9303 - val_loss: 5.1836\n",
      "Epoch 453/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9592 - val_loss: 5.1532\n",
      "Epoch 454/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9324 - val_loss: 5.1379\n",
      "Epoch 455/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9440 - val_loss: 5.2566\n",
      "Epoch 456/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9218 - val_loss: 5.1172\n",
      "Epoch 457/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9847 - val_loss: 5.1420\n",
      "Epoch 458/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9385 - val_loss: 5.0679\n",
      "Epoch 459/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8965 - val_loss: 5.3332\n",
      "Epoch 460/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9215 - val_loss: 5.2002\n",
      "Epoch 461/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9329 - val_loss: 5.0670\n",
      "Epoch 462/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8908 - val_loss: 5.1185\n",
      "Epoch 463/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9488 - val_loss: 5.1575\n",
      "Epoch 464/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9211 - val_loss: 5.1874\n",
      "Epoch 465/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9418 - val_loss: 5.2731\n",
      "Epoch 466/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9452 - val_loss: 5.1618\n",
      "Epoch 467/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9441 - val_loss: 5.3695\n",
      "Epoch 468/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9121 - val_loss: 5.3379\n",
      "Epoch 469/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9257 - val_loss: 5.1177\n",
      "Epoch 470/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9461 - val_loss: 5.1554\n",
      "Epoch 471/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9050 - val_loss: 5.0837\n",
      "Epoch 472/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9205 - val_loss: 5.0682\n",
      "Epoch 473/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0052 - val_loss: 5.2462\n",
      "Epoch 474/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9246 - val_loss: 5.1251\n",
      "Epoch 475/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9275 - val_loss: 5.1273\n",
      "Epoch 476/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9182 - val_loss: 5.3212\n",
      "Epoch 477/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9060 - val_loss: 5.1757\n",
      "Epoch 478/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9247 - val_loss: 5.5107\n",
      "Epoch 479/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9225 - val_loss: 5.3202\n",
      "Epoch 480/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9250 - val_loss: 5.2054\n",
      "Epoch 481/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9211 - val_loss: 5.1860\n",
      "Epoch 482/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9562 - val_loss: 5.3618\n",
      "Epoch 483/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9150 - val_loss: 5.0938\n",
      "Epoch 484/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9369 - val_loss: 5.1397\n",
      "Epoch 485/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9678 - val_loss: 5.1037\n",
      "Epoch 486/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0050 - val_loss: 5.1644\n",
      "Epoch 487/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9585 - val_loss: 5.2754\n",
      "Epoch 488/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9232 - val_loss: 5.0893\n",
      "Epoch 489/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9306 - val_loss: 5.0863\n",
      "Epoch 490/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9093 - val_loss: 5.0821\n",
      "Epoch 491/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9460 - val_loss: 5.2123\n",
      "Epoch 492/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9075 - val_loss: 5.1777\n",
      "Epoch 493/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9477 - val_loss: 5.1701\n",
      "Epoch 494/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9214 - val_loss: 5.2098\n",
      "Epoch 495/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9016 - val_loss: 5.0968\n",
      "Epoch 496/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8799 - val_loss: 5.1018\n",
      "Epoch 497/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9200 - val_loss: 5.1095\n",
      "Epoch 498/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9205 - val_loss: 5.1896\n",
      "Epoch 499/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9037 - val_loss: 5.1379\n",
      "Epoch 500/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9325 - val_loss: 5.3585\n",
      "Epoch 501/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9370 - val_loss: 5.0536\n",
      "Epoch 502/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9820 - val_loss: 5.3488\n",
      "Epoch 503/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9362 - val_loss: 5.1170\n",
      "Epoch 504/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9380 - val_loss: 5.1249\n",
      "Epoch 505/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9116 - val_loss: 5.0556\n",
      "Epoch 506/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9120 - val_loss: 5.0925\n",
      "Epoch 507/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9257 - val_loss: 5.1508\n",
      "Epoch 508/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9261 - val_loss: 5.1545\n",
      "Epoch 509/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9331 - val_loss: 5.1609\n",
      "Epoch 510/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9159 - val_loss: 5.1234\n",
      "Epoch 511/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9144 - val_loss: 5.1233\n",
      "Epoch 512/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0064 - val_loss: 5.1195\n",
      "Epoch 513/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9728 - val_loss: 4.9866\n",
      "Epoch 514/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9201 - val_loss: 5.0619\n",
      "Epoch 515/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9229 - val_loss: 5.3735\n",
      "Epoch 516/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8965 - val_loss: 5.1531\n",
      "Epoch 517/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9338 - val_loss: 5.1114\n",
      "Epoch 518/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9138 - val_loss: 5.1151\n",
      "Epoch 519/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9061 - val_loss: 5.1006\n",
      "Epoch 520/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9073 - val_loss: 5.0843\n",
      "Epoch 521/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9160 - val_loss: 5.2408\n",
      "Epoch 522/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9422 - val_loss: 5.1305\n",
      "Epoch 523/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 5.0021 - val_loss: 5.2163\n",
      "Epoch 524/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8860 - val_loss: 5.1362\n",
      "Epoch 525/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9128 - val_loss: 5.2716\n",
      "Epoch 526/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9500 - val_loss: 5.0977\n",
      "Epoch 527/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9117 - val_loss: 5.1224\n",
      "Epoch 528/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9044 - val_loss: 5.1360\n",
      "Epoch 529/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9087 - val_loss: 5.0602\n",
      "Epoch 530/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9001 - val_loss: 5.1600\n",
      "Epoch 531/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9441 - val_loss: 5.1129\n",
      "Epoch 532/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9033 - val_loss: 5.1365\n",
      "Epoch 533/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9306 - val_loss: 5.1736\n",
      "Epoch 534/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8868 - val_loss: 5.2162\n",
      "Epoch 535/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9119 - val_loss: 5.2102\n",
      "Epoch 536/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9249 - val_loss: 5.2224\n",
      "Epoch 537/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8982 - val_loss: 5.3130\n",
      "Epoch 538/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9378 - val_loss: 5.4815\n",
      "Epoch 539/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9359 - val_loss: 5.5585\n",
      "Epoch 540/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9793 - val_loss: 5.1525\n",
      "Epoch 541/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8875 - val_loss: 5.1409\n",
      "Epoch 542/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8968 - val_loss: 5.1044\n",
      "Epoch 543/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9021 - val_loss: 5.1883\n",
      "Epoch 544/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9448 - val_loss: 5.1640\n",
      "Epoch 545/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9105 - val_loss: 5.1213\n",
      "Epoch 546/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9087 - val_loss: 5.1231\n",
      "Epoch 547/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8992 - val_loss: 5.1947\n",
      "Epoch 548/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8898 - val_loss: 5.1408\n",
      "Epoch 549/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9231 - val_loss: 5.1682\n",
      "Epoch 550/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8944 - val_loss: 5.1606\n",
      "Epoch 551/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9122 - val_loss: 5.0386\n",
      "Epoch 552/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8647 - val_loss: 5.1579\n",
      "Epoch 553/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8990 - val_loss: 5.1783\n",
      "Epoch 554/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9353 - val_loss: 5.0657\n",
      "Epoch 555/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9201 - val_loss: 5.1459\n",
      "Epoch 556/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9265 - val_loss: 5.1816\n",
      "Epoch 557/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8780 - val_loss: 5.1997\n",
      "Epoch 558/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9279 - val_loss: 5.1395\n",
      "Epoch 559/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8863 - val_loss: 5.0975\n",
      "Epoch 560/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9226 - val_loss: 5.1860\n",
      "Epoch 561/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9346 - val_loss: 5.1209\n",
      "Epoch 562/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9101 - val_loss: 5.1891\n",
      "Epoch 563/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9107 - val_loss: 5.1437\n",
      "Epoch 564/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9171 - val_loss: 5.0887\n",
      "Epoch 565/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9119 - val_loss: 5.0685\n",
      "Epoch 566/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9786 - val_loss: 5.1974\n",
      "Epoch 567/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9106 - val_loss: 5.0732\n",
      "Epoch 568/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8901 - val_loss: 5.1937\n",
      "Epoch 569/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9543 - val_loss: 5.1389\n",
      "Epoch 570/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9273 - val_loss: 5.1691\n",
      "Epoch 571/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8727 - val_loss: 5.1023\n",
      "Epoch 572/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9472 - val_loss: 5.1349\n",
      "Epoch 573/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9126 - val_loss: 5.3701\n",
      "Epoch 574/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9581 - val_loss: 5.2566\n",
      "Epoch 575/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9134 - val_loss: 5.1808\n",
      "Epoch 576/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8934 - val_loss: 5.1311\n",
      "Epoch 577/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9098 - val_loss: 5.1742\n",
      "Epoch 578/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9044 - val_loss: 5.2279\n",
      "Epoch 579/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8749 - val_loss: 5.1197\n",
      "Epoch 580/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8877 - val_loss: 5.2386\n",
      "Epoch 581/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9181 - val_loss: 5.0811\n",
      "Epoch 582/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9258 - val_loss: 5.1067\n",
      "Epoch 583/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9068 - val_loss: 5.2546\n",
      "Epoch 584/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9061 - val_loss: 5.1832\n",
      "Epoch 585/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9103 - val_loss: 5.1660\n",
      "Epoch 586/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8577 - val_loss: 5.2739\n",
      "Epoch 587/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8793 - val_loss: 5.0901\n",
      "Epoch 588/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8658 - val_loss: 5.0957\n",
      "Epoch 589/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9081 - val_loss: 5.1378\n",
      "Epoch 590/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8853 - val_loss: 5.1645\n",
      "Epoch 591/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8799 - val_loss: 5.0539\n",
      "Epoch 592/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8770 - val_loss: 5.4417\n",
      "Epoch 593/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8878 - val_loss: 5.2342\n",
      "Epoch 594/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8794 - val_loss: 5.0727\n",
      "Epoch 595/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9212 - val_loss: 5.1213\n",
      "Epoch 596/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8953 - val_loss: 5.3185\n",
      "Epoch 597/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9255 - val_loss: 5.2858\n",
      "Epoch 598/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8669 - val_loss: 5.2256\n",
      "Epoch 599/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9072 - val_loss: 5.4056\n",
      "Epoch 600/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9513 - val_loss: 5.4366\n",
      "Epoch 601/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9054 - val_loss: 5.1380\n",
      "Epoch 602/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8841 - val_loss: 5.1289\n",
      "Epoch 603/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8956 - val_loss: 5.0803\n",
      "Epoch 604/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8879 - val_loss: 5.3297\n",
      "Epoch 605/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8761 - val_loss: 5.3636\n",
      "Epoch 606/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8799 - val_loss: 5.4506\n",
      "Epoch 607/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9379 - val_loss: 5.1788\n",
      "Epoch 608/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8786 - val_loss: 5.2036\n",
      "Epoch 609/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9002 - val_loss: 5.4924\n",
      "Epoch 610/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8769 - val_loss: 5.2810\n",
      "Epoch 611/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8664 - val_loss: 5.4280\n",
      "Epoch 612/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8829 - val_loss: 5.1276\n",
      "Epoch 613/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8601 - val_loss: 5.1130\n",
      "Epoch 614/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8894 - val_loss: 5.1401\n",
      "Epoch 615/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9439 - val_loss: 5.1682\n",
      "Epoch 616/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8700 - val_loss: 5.0820\n",
      "Epoch 617/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8816 - val_loss: 5.1353\n",
      "Epoch 618/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9029 - val_loss: 5.2074\n",
      "Epoch 619/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9093 - val_loss: 5.2799\n",
      "Epoch 620/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8961 - val_loss: 5.1006\n",
      "Epoch 621/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9177 - val_loss: 5.0926\n",
      "Epoch 622/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8882 - val_loss: 5.1087\n",
      "Epoch 623/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8735 - val_loss: 5.1739\n",
      "Epoch 624/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8973 - val_loss: 5.2375\n",
      "Epoch 625/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.9987 - val_loss: 5.2512\n",
      "Epoch 626/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8769 - val_loss: 5.2611\n",
      "Epoch 627/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8850 - val_loss: 5.5450\n",
      "Epoch 628/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9150 - val_loss: 5.1210\n",
      "Epoch 629/10000\n",
      "707/707 [==============================] - 1s 1000us/step - loss: 4.9103 - val_loss: 5.1566\n",
      "Epoch 630/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9416 - val_loss: 5.5244\n",
      "Epoch 631/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8788 - val_loss: 5.2018\n",
      "Epoch 632/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8889 - val_loss: 5.2168\n",
      "Epoch 633/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8755 - val_loss: 5.2746\n",
      "Epoch 634/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8966 - val_loss: 5.4286\n",
      "Epoch 635/10000\n",
      "707/707 [==============================] - 1s 1000us/step - loss: 4.8783 - val_loss: 5.2121\n",
      "Epoch 636/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.9264 - val_loss: 5.1118\n",
      "Epoch 637/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8675 - val_loss: 5.1078\n",
      "Epoch 638/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8797 - val_loss: 5.1787\n",
      "Epoch 639/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8451 - val_loss: 5.3017\n",
      "Epoch 640/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.9423 - val_loss: 5.1672\n",
      "Epoch 641/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8843 - val_loss: 5.1801\n",
      "Epoch 642/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.9130 - val_loss: 5.1849\n",
      "Epoch 643/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.8901 - val_loss: 5.1909\n",
      "Epoch 644/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8735 - val_loss: 5.1039\n",
      "Epoch 645/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9116 - val_loss: 5.1661\n",
      "Epoch 646/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9245 - val_loss: 5.1724\n",
      "Epoch 647/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8941 - val_loss: 5.1151\n",
      "Epoch 648/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8791 - val_loss: 5.1232\n",
      "Epoch 649/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8469 - val_loss: 5.1320\n",
      "Epoch 650/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8810 - val_loss: 5.2259\n",
      "Epoch 651/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.9087 - val_loss: 5.1471\n",
      "Epoch 652/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8751 - val_loss: 5.1656\n",
      "Epoch 653/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8790 - val_loss: 5.1496\n",
      "Epoch 654/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8674 - val_loss: 5.2507\n",
      "Epoch 655/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9057 - val_loss: 5.1931\n",
      "Epoch 656/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8424 - val_loss: 5.1066\n",
      "Epoch 657/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8569 - val_loss: 5.1758\n",
      "Epoch 658/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.9119 - val_loss: 5.1976\n",
      "Epoch 659/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8979 - val_loss: 5.2673\n",
      "Epoch 660/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8841 - val_loss: 5.2746\n",
      "Epoch 661/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8802 - val_loss: 5.1379\n",
      "Epoch 662/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.9639 - val_loss: 5.1476\n",
      "Epoch 663/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8410 - val_loss: 5.0687\n",
      "Epoch 664/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8677 - val_loss: 5.1287\n",
      "Epoch 665/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8947 - val_loss: 5.1102\n",
      "Epoch 666/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8630 - val_loss: 5.1974\n",
      "Epoch 667/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8780 - val_loss: 5.1701\n",
      "Epoch 668/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8678 - val_loss: 5.1558\n",
      "Epoch 669/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9264 - val_loss: 5.1428\n",
      "Epoch 670/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9217 - val_loss: 5.2409\n",
      "Epoch 671/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8785 - val_loss: 5.2517\n",
      "Epoch 672/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8733 - val_loss: 5.3181\n",
      "Epoch 673/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8921 - val_loss: 5.1773\n",
      "Epoch 674/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8662 - val_loss: 5.2233\n",
      "Epoch 675/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8756 - val_loss: 5.1531\n",
      "Epoch 676/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8872 - val_loss: 5.0718\n",
      "Epoch 677/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8737 - val_loss: 5.1092\n",
      "Epoch 678/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8720 - val_loss: 5.2553\n",
      "Epoch 679/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8773 - val_loss: 5.1350\n",
      "Epoch 680/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8651 - val_loss: 5.2223\n",
      "Epoch 681/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8569 - val_loss: 5.1291\n",
      "Epoch 682/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.8741 - val_loss: 5.2333\n",
      "Epoch 683/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8627 - val_loss: 5.1144\n",
      "Epoch 684/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8762 - val_loss: 5.1187\n",
      "Epoch 685/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.8469 - val_loss: 5.1252\n",
      "Epoch 686/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8635 - val_loss: 5.1156\n",
      "Epoch 687/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.8787 - val_loss: 5.2442\n",
      "Epoch 688/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8432 - val_loss: 5.2394\n",
      "Epoch 689/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8899 - val_loss: 5.1540\n",
      "Epoch 690/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8622 - val_loss: 5.3148\n",
      "Epoch 691/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9041 - val_loss: 5.0966\n",
      "Epoch 692/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.8987 - val_loss: 5.2222\n",
      "Epoch 693/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8712 - val_loss: 5.0867\n",
      "Epoch 694/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8502 - val_loss: 5.1480\n",
      "Epoch 695/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8412 - val_loss: 5.1069\n",
      "Epoch 696/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8567 - val_loss: 5.1159\n",
      "Epoch 697/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9043 - val_loss: 5.1534\n",
      "Epoch 698/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8787 - val_loss: 5.1285\n",
      "Epoch 699/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8972 - val_loss: 5.3010\n",
      "Epoch 700/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8747 - val_loss: 5.0948\n",
      "Epoch 701/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.8802 - val_loss: 5.0598\n",
      "Epoch 702/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8751 - val_loss: 5.2179\n",
      "Epoch 703/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8809 - val_loss: 5.1589\n",
      "Epoch 704/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8553 - val_loss: 5.2466\n",
      "Epoch 705/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8662 - val_loss: 5.2405\n",
      "Epoch 706/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8893 - val_loss: 5.2487\n",
      "Epoch 707/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8416 - val_loss: 5.6495\n",
      "Epoch 708/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8902 - val_loss: 5.1694\n",
      "Epoch 709/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8463 - val_loss: 5.2617\n",
      "Epoch 710/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8938 - val_loss: 5.1623\n",
      "Epoch 711/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8762 - val_loss: 5.1388\n",
      "Epoch 712/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8617 - val_loss: 5.2126\n",
      "Epoch 713/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8511 - val_loss: 5.1624\n",
      "Epoch 714/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8540 - val_loss: 5.1629\n",
      "Epoch 715/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8741 - val_loss: 5.1152\n",
      "Epoch 716/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9043 - val_loss: 5.3983\n",
      "Epoch 717/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8544 - val_loss: 5.1050\n",
      "Epoch 718/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8573 - val_loss: 5.3365\n",
      "Epoch 719/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.8726 - val_loss: 5.2094\n",
      "Epoch 720/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8638 - val_loss: 5.2056\n",
      "Epoch 721/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8455 - val_loss: 5.1215\n",
      "Epoch 722/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8585 - val_loss: 5.1179\n",
      "Epoch 723/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8363 - val_loss: 5.0874\n",
      "Epoch 724/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8775 - val_loss: 5.3630\n",
      "Epoch 725/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8386 - val_loss: 5.2816\n",
      "Epoch 726/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8864 - val_loss: 5.1574\n",
      "Epoch 727/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8407 - val_loss: 5.1895\n",
      "Epoch 728/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8560 - val_loss: 5.2571\n",
      "Epoch 729/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8676 - val_loss: 5.1694\n",
      "Epoch 730/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8208 - val_loss: 5.2644\n",
      "Epoch 731/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8416 - val_loss: 5.1424\n",
      "Epoch 732/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8948 - val_loss: 5.1738\n",
      "Epoch 733/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8489 - val_loss: 5.1270\n",
      "Epoch 734/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8559 - val_loss: 5.4399\n",
      "Epoch 735/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8829 - val_loss: 5.1038\n",
      "Epoch 736/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8409 - val_loss: 5.1632\n",
      "Epoch 737/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8431 - val_loss: 5.1666\n",
      "Epoch 738/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8655 - val_loss: 5.0958\n",
      "Epoch 739/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8263 - val_loss: 5.1400\n",
      "Epoch 740/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.9038 - val_loss: 5.1605\n",
      "Epoch 741/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8407 - val_loss: 5.2364\n",
      "Epoch 742/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.8616 - val_loss: 5.2156\n",
      "Epoch 743/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8435 - val_loss: 5.3044\n",
      "Epoch 744/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8395 - val_loss: 5.2459\n",
      "Epoch 745/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8551 - val_loss: 5.2565\n",
      "Epoch 746/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8384 - val_loss: 5.1351\n",
      "Epoch 747/10000\n",
      "707/707 [==============================] - 1s 1000us/step - loss: 4.8914 - val_loss: 5.2737\n",
      "Epoch 748/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8253 - val_loss: 5.3790\n",
      "Epoch 749/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.9016 - val_loss: 5.4248\n",
      "Epoch 750/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8617 - val_loss: 5.1363\n",
      "Epoch 751/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8306 - val_loss: 5.7973\n",
      "Epoch 752/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.8448 - val_loss: 5.1660\n",
      "Epoch 753/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8370 - val_loss: 5.2025\n",
      "Epoch 754/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.8620 - val_loss: 5.1681\n",
      "Epoch 755/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8399 - val_loss: 5.1532\n",
      "Epoch 756/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.8610 - val_loss: 5.1303\n",
      "Epoch 757/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8361 - val_loss: 5.1416\n",
      "Epoch 758/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8485 - val_loss: 5.2481\n",
      "Epoch 759/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8973 - val_loss: 5.1669\n",
      "Epoch 760/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8378 - val_loss: 5.2375\n",
      "Epoch 761/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8432 - val_loss: 5.1730\n",
      "Epoch 762/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8438 - val_loss: 5.3222\n",
      "Epoch 763/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8545 - val_loss: 5.1636\n",
      "Epoch 764/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8482 - val_loss: 5.2131\n",
      "Epoch 765/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8803 - val_loss: 5.3580\n",
      "Epoch 766/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8210 - val_loss: 5.1430\n",
      "Epoch 767/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8570 - val_loss: 5.1903\n",
      "Epoch 768/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8638 - val_loss: 5.1581\n",
      "Epoch 769/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8572 - val_loss: 5.1427\n",
      "Epoch 770/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8784 - val_loss: 5.1826\n",
      "Epoch 771/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8225 - val_loss: 5.2126\n",
      "Epoch 772/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8715 - val_loss: 5.1484\n",
      "Epoch 773/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8251 - val_loss: 5.3002\n",
      "Epoch 774/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8363 - val_loss: 5.1827\n",
      "Epoch 775/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8451 - val_loss: 5.1953\n",
      "Epoch 776/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.8616 - val_loss: 5.1814\n",
      "Epoch 777/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8326 - val_loss: 5.1178\n",
      "Epoch 778/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8262 - val_loss: 5.2101\n",
      "Epoch 779/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8381 - val_loss: 5.1839\n",
      "Epoch 780/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8372 - val_loss: 5.1128\n",
      "Epoch 781/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8635 - val_loss: 5.2952\n",
      "Epoch 782/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8381 - val_loss: 5.1122\n",
      "Epoch 783/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8630 - val_loss: 5.2437\n",
      "Epoch 784/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8763 - val_loss: 5.3539\n",
      "Epoch 785/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8555 - val_loss: 5.1280\n",
      "Epoch 786/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.8372 - val_loss: 5.2636\n",
      "Epoch 787/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8204 - val_loss: 5.1921\n",
      "Epoch 788/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8469 - val_loss: 5.2820\n",
      "Epoch 789/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.8562 - val_loss: 5.2314\n",
      "Epoch 790/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8184 - val_loss: 5.2192\n",
      "Epoch 791/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8373 - val_loss: 5.4262\n",
      "Epoch 792/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8220 - val_loss: 5.2021\n",
      "Epoch 793/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8197 - val_loss: 5.1290\n",
      "Epoch 794/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8324 - val_loss: 5.1927\n",
      "Epoch 795/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8239 - val_loss: 5.1743\n",
      "Epoch 796/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8555 - val_loss: 5.2703\n",
      "Epoch 797/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8655 - val_loss: 5.1480\n",
      "Epoch 798/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8167 - val_loss: 5.2382\n",
      "Epoch 799/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8431 - val_loss: 5.3215\n",
      "Epoch 800/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8326 - val_loss: 5.2838\n",
      "Epoch 801/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8568 - val_loss: 5.3536\n",
      "Epoch 802/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.8354 - val_loss: 5.2302\n",
      "Epoch 803/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8822 - val_loss: 5.2117\n",
      "Epoch 804/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8132 - val_loss: 5.2230\n",
      "Epoch 805/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8158 - val_loss: 5.1657\n",
      "Epoch 806/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8282 - val_loss: 5.4749\n",
      "Epoch 807/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8440 - val_loss: 5.3100\n",
      "Epoch 808/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8303 - val_loss: 5.4740\n",
      "Epoch 809/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8243 - val_loss: 5.1889\n",
      "Epoch 810/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8327 - val_loss: 5.2699\n",
      "Epoch 811/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8267 - val_loss: 5.2029\n",
      "Epoch 812/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.8099 - val_loss: 5.3796\n",
      "Epoch 813/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8212 - val_loss: 5.1347\n",
      "Epoch 814/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8630 - val_loss: 5.6334\n",
      "Epoch 815/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8457 - val_loss: 5.1967\n",
      "Epoch 816/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8299 - val_loss: 5.3581\n",
      "Epoch 817/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8516 - val_loss: 5.2382\n",
      "Epoch 818/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8180 - val_loss: 5.2889\n",
      "Epoch 819/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8424 - val_loss: 5.2303\n",
      "Epoch 820/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.8153 - val_loss: 5.2572\n",
      "Epoch 821/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8272 - val_loss: 5.1862\n",
      "Epoch 822/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8426 - val_loss: 5.2676\n",
      "Epoch 823/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8197 - val_loss: 5.1256\n",
      "Epoch 824/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8461 - val_loss: 5.1411\n",
      "Epoch 825/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8437 - val_loss: 5.2555\n",
      "Epoch 826/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8299 - val_loss: 5.2995\n",
      "Epoch 827/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.8199 - val_loss: 5.1118\n",
      "Epoch 828/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.8352 - val_loss: 5.3845\n",
      "Epoch 829/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8145 - val_loss: 5.3116\n",
      "Epoch 830/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.8255 - val_loss: 5.2626\n",
      "Epoch 831/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8625 - val_loss: 5.1977\n",
      "Epoch 832/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8172 - val_loss: 5.2200\n",
      "Epoch 833/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8527 - val_loss: 5.1728\n",
      "Epoch 834/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8523 - val_loss: 5.1554\n",
      "Epoch 835/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8342 - val_loss: 5.2119\n",
      "Epoch 836/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.7986 - val_loss: 5.1729\n",
      "Epoch 837/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8185 - val_loss: 5.2068\n",
      "Epoch 838/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8384 - val_loss: 5.1215\n",
      "Epoch 839/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8629 - val_loss: 5.1921\n",
      "Epoch 840/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8333 - val_loss: 5.1777\n",
      "Epoch 841/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8309 - val_loss: 5.2097\n",
      "Epoch 842/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8527 - val_loss: 5.3114\n",
      "Epoch 843/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8191 - val_loss: 5.2796\n",
      "Epoch 844/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8170 - val_loss: 5.2183\n",
      "Epoch 845/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8268 - val_loss: 5.3112\n",
      "Epoch 846/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8310 - val_loss: 5.2454\n",
      "Epoch 847/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8208 - val_loss: 5.1369\n",
      "Epoch 848/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8007 - val_loss: 5.1649\n",
      "Epoch 849/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8153 - val_loss: 5.4864\n",
      "Epoch 850/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8576 - val_loss: 5.4003\n",
      "Epoch 851/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8240 - val_loss: 5.2177\n",
      "Epoch 852/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8119 - val_loss: 5.2087\n",
      "Epoch 853/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8485 - val_loss: 5.2169\n",
      "Epoch 854/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8179 - val_loss: 5.2625\n",
      "Epoch 855/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8252 - val_loss: 5.3169\n",
      "Epoch 856/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8100 - val_loss: 5.2375\n",
      "Epoch 857/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8049 - val_loss: 5.4131\n",
      "Epoch 858/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8296 - val_loss: 5.5023\n",
      "Epoch 859/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8284 - val_loss: 5.3031\n",
      "Epoch 860/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8936 - val_loss: 5.1919\n",
      "Epoch 861/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8004 - val_loss: 5.2537\n",
      "Epoch 862/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8211 - val_loss: 5.1382\n",
      "Epoch 863/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8179 - val_loss: 5.3250\n",
      "Epoch 864/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8105 - val_loss: 5.2301\n",
      "Epoch 865/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8228 - val_loss: 5.2736\n",
      "Epoch 866/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8250 - val_loss: 5.1793\n",
      "Epoch 867/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8047 - val_loss: 5.2061\n",
      "Epoch 868/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.8534 - val_loss: 5.1168\n",
      "Epoch 869/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7952 - val_loss: 5.4476\n",
      "Epoch 870/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8176 - val_loss: 5.2087\n",
      "Epoch 871/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7949 - val_loss: 5.1403\n",
      "Epoch 872/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8217 - val_loss: 5.1865\n",
      "Epoch 873/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8116 - val_loss: 5.2468\n",
      "Epoch 874/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8144 - val_loss: 5.1518\n",
      "Epoch 875/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8186 - val_loss: 5.2481\n",
      "Epoch 876/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.8348 - val_loss: 5.1731\n",
      "Epoch 877/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.8034 - val_loss: 5.3091\n",
      "Epoch 878/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8146 - val_loss: 5.1637\n",
      "Epoch 879/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.8120 - val_loss: 5.1293\n",
      "Epoch 880/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8158 - val_loss: 5.2225\n",
      "Epoch 881/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8493 - val_loss: 5.2168\n",
      "Epoch 882/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8261 - val_loss: 5.2621\n",
      "Epoch 883/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.8039 - val_loss: 5.1511\n",
      "Epoch 884/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8103 - val_loss: 5.2597\n",
      "Epoch 885/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8098 - val_loss: 5.0688\n",
      "Epoch 886/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.8095 - val_loss: 5.2205\n",
      "Epoch 887/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.8022 - val_loss: 5.4663\n",
      "Epoch 888/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8324 - val_loss: 5.2362\n",
      "Epoch 889/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.8175 - val_loss: 5.3192\n",
      "Epoch 890/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8085 - val_loss: 5.2777\n",
      "Epoch 891/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8234 - val_loss: 5.2549\n",
      "Epoch 892/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8032 - val_loss: 5.1337\n",
      "Epoch 893/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7968 - val_loss: 5.2244\n",
      "Epoch 894/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8216 - val_loss: 5.1516\n",
      "Epoch 895/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8136 - val_loss: 5.5714\n",
      "Epoch 896/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7937 - val_loss: 5.2899\n",
      "Epoch 897/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.8181 - val_loss: 5.3338\n",
      "Epoch 898/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7864 - val_loss: 5.2247\n",
      "Epoch 899/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7850 - val_loss: 5.3375\n",
      "Epoch 900/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.8081 - val_loss: 5.1795\n",
      "Epoch 901/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8309 - val_loss: 5.1650\n",
      "Epoch 902/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8079 - val_loss: 5.1799\n",
      "Epoch 903/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8139 - val_loss: 5.1713\n",
      "Epoch 904/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8022 - val_loss: 5.3337\n",
      "Epoch 905/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7923 - val_loss: 5.2055\n",
      "Epoch 906/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8205 - val_loss: 5.3119\n",
      "Epoch 907/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7872 - val_loss: 5.3008\n",
      "Epoch 908/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8236 - val_loss: 5.6821\n",
      "Epoch 909/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8025 - val_loss: 5.2333\n",
      "Epoch 910/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8032 - val_loss: 5.3739\n",
      "Epoch 911/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.8148 - val_loss: 5.1688\n",
      "Epoch 912/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7983 - val_loss: 5.2220\n",
      "Epoch 913/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8061 - val_loss: 5.3869\n",
      "Epoch 914/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.7984 - val_loss: 5.1914\n",
      "Epoch 915/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7745 - val_loss: 5.2591\n",
      "Epoch 916/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8041 - val_loss: 5.2541\n",
      "Epoch 917/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7995 - val_loss: 5.2054\n",
      "Epoch 918/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8177 - val_loss: 5.2380\n",
      "Epoch 919/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8179 - val_loss: 5.3539\n",
      "Epoch 920/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8060 - val_loss: 5.2564\n",
      "Epoch 921/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8031 - val_loss: 5.1398\n",
      "Epoch 922/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8160 - val_loss: 5.2359\n",
      "Epoch 923/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.8130 - val_loss: 5.1922\n",
      "Epoch 924/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8244 - val_loss: 5.2061\n",
      "Epoch 925/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.8135 - val_loss: 5.1942\n",
      "Epoch 926/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7946 - val_loss: 5.3263\n",
      "Epoch 927/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8337 - val_loss: 5.1662\n",
      "Epoch 928/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8155 - val_loss: 5.2626\n",
      "Epoch 929/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.8048 - val_loss: 5.3349\n",
      "Epoch 930/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8100 - val_loss: 5.3661\n",
      "Epoch 931/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8227 - val_loss: 5.1994\n",
      "Epoch 932/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8002 - val_loss: 5.3330\n",
      "Epoch 933/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7961 - val_loss: 5.2251\n",
      "Epoch 934/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.8139 - val_loss: 5.2345\n",
      "Epoch 935/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7959 - val_loss: 5.2229\n",
      "Epoch 936/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8100 - val_loss: 5.2012\n",
      "Epoch 937/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7950 - val_loss: 5.1857\n",
      "Epoch 938/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8032 - val_loss: 5.3071\n",
      "Epoch 939/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8067 - val_loss: 5.2676\n",
      "Epoch 940/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8137 - val_loss: 5.1651\n",
      "Epoch 941/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8018 - val_loss: 5.3079\n",
      "Epoch 942/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7952 - val_loss: 5.2303\n",
      "Epoch 943/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7968 - val_loss: 5.2644\n",
      "Epoch 944/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8052 - val_loss: 5.1662\n",
      "Epoch 945/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7990 - val_loss: 5.1885\n",
      "Epoch 946/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7970 - val_loss: 5.1465\n",
      "Epoch 947/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8068 - val_loss: 5.2883\n",
      "Epoch 948/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8133 - val_loss: 5.2064\n",
      "Epoch 949/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7908 - val_loss: 5.1207\n",
      "Epoch 950/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.8088 - val_loss: 5.1868\n",
      "Epoch 951/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7981 - val_loss: 5.2305\n",
      "Epoch 952/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7986 - val_loss: 5.2987\n",
      "Epoch 953/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7993 - val_loss: 5.1998\n",
      "Epoch 954/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8178 - val_loss: 5.2295\n",
      "Epoch 955/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7864 - val_loss: 5.2118\n",
      "Epoch 956/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.7796 - val_loss: 5.2258\n",
      "Epoch 957/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7804 - val_loss: 5.2600\n",
      "Epoch 958/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7897 - val_loss: 5.1363\n",
      "Epoch 959/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7991 - val_loss: 5.2165\n",
      "Epoch 960/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7959 - val_loss: 5.1952\n",
      "Epoch 961/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8112 - val_loss: 5.3485\n",
      "Epoch 962/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8035 - val_loss: 5.2344\n",
      "Epoch 963/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7965 - val_loss: 5.2480\n",
      "Epoch 964/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7838 - val_loss: 5.2758\n",
      "Epoch 965/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8082 - val_loss: 5.1978\n",
      "Epoch 966/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7934 - val_loss: 5.2266\n",
      "Epoch 967/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7726 - val_loss: 5.2381\n",
      "Epoch 968/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.8277 - val_loss: 5.2980\n",
      "Epoch 969/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7913 - val_loss: 5.5168\n",
      "Epoch 970/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7831 - val_loss: 5.2600\n",
      "Epoch 971/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7795 - val_loss: 5.3209\n",
      "Epoch 972/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7786 - val_loss: 5.2415\n",
      "Epoch 973/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7855 - val_loss: 5.4154\n",
      "Epoch 974/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.7875 - val_loss: 5.4212\n",
      "Epoch 975/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7868 - val_loss: 5.2286\n",
      "Epoch 976/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7892 - val_loss: 5.1999\n",
      "Epoch 977/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7873 - val_loss: 5.2341\n",
      "Epoch 978/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7870 - val_loss: 5.2632\n",
      "Epoch 979/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7866 - val_loss: 5.2397\n",
      "Epoch 980/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7812 - val_loss: 5.1821\n",
      "Epoch 981/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7787 - val_loss: 5.2005\n",
      "Epoch 982/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7804 - val_loss: 5.2636\n",
      "Epoch 983/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7937 - val_loss: 5.5280\n",
      "Epoch 984/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7894 - val_loss: 5.1578\n",
      "Epoch 985/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7907 - val_loss: 5.1538\n",
      "Epoch 986/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7858 - val_loss: 5.2960\n",
      "Epoch 987/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8072 - val_loss: 5.3087\n",
      "Epoch 988/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7870 - val_loss: 5.2007\n",
      "Epoch 989/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8166 - val_loss: 5.5085\n",
      "Epoch 990/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7971 - val_loss: 5.3067\n",
      "Epoch 991/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7791 - val_loss: 5.2524\n",
      "Epoch 992/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8021 - val_loss: 5.1355\n",
      "Epoch 993/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7880 - val_loss: 5.2473\n",
      "Epoch 994/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8153 - val_loss: 5.1291\n",
      "Epoch 995/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7854 - val_loss: 5.2316\n",
      "Epoch 996/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8159 - val_loss: 5.3628\n",
      "Epoch 997/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7942 - val_loss: 5.2343\n",
      "Epoch 998/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7991 - val_loss: 5.2080\n",
      "Epoch 999/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7767 - val_loss: 5.2105\n",
      "Epoch 1000/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7904 - val_loss: 5.3952\n",
      "Epoch 1001/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8043 - val_loss: 5.2087\n",
      "Epoch 1002/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7890 - val_loss: 5.1846\n",
      "Epoch 1003/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7966 - val_loss: 5.2642\n",
      "Epoch 1004/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7896 - val_loss: 5.2163\n",
      "Epoch 1005/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7901 - val_loss: 5.3311\n",
      "Epoch 1006/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7734 - val_loss: 5.3487\n",
      "Epoch 1007/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7885 - val_loss: 5.1373\n",
      "Epoch 1008/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.8196 - val_loss: 5.3393\n",
      "Epoch 1009/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7879 - val_loss: 5.2377\n",
      "Epoch 1010/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.8172 - val_loss: 5.2836\n",
      "Epoch 1011/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7912 - val_loss: 5.2125\n",
      "Epoch 1012/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.8002 - val_loss: 5.2858\n",
      "Epoch 1013/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7861 - val_loss: 5.1634\n",
      "Epoch 1014/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7721 - val_loss: 5.2602\n",
      "Epoch 1015/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.7782 - val_loss: 5.2655\n",
      "Epoch 1016/10000\n",
      "707/707 [==============================] - 1s 963us/step - loss: 4.7942 - val_loss: 5.2647\n",
      "Epoch 1017/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.8001 - val_loss: 5.1929\n",
      "Epoch 1018/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7834 - val_loss: 5.2113\n",
      "Epoch 1019/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7741 - val_loss: 5.2577\n",
      "Epoch 1020/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7703 - val_loss: 5.2417\n",
      "Epoch 1021/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7925 - val_loss: 5.2085\n",
      "Epoch 1022/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7878 - val_loss: 5.2609\n",
      "Epoch 1023/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7904 - val_loss: 5.2848\n",
      "Epoch 1024/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.8045 - val_loss: 5.2601\n",
      "Epoch 1025/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7871 - val_loss: 5.2013\n",
      "Epoch 1026/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.8022 - val_loss: 5.2541\n",
      "Epoch 1027/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7734 - val_loss: 5.2674\n",
      "Epoch 1028/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7822 - val_loss: 5.2376\n",
      "Epoch 1029/10000\n",
      "707/707 [==============================] - 1s 967us/step - loss: 4.7607 - val_loss: 5.2823\n",
      "Epoch 1030/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7935 - val_loss: 5.2971\n",
      "Epoch 1031/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7745 - val_loss: 5.7561\n",
      "Epoch 1032/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7557 - val_loss: 5.1920\n",
      "Epoch 1033/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7656 - val_loss: 5.2252\n",
      "Epoch 1034/10000\n",
      "707/707 [==============================] - 1s 963us/step - loss: 4.7809 - val_loss: 5.2566\n",
      "Epoch 1035/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7818 - val_loss: 5.1902\n",
      "Epoch 1036/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7799 - val_loss: 5.2076\n",
      "Epoch 1037/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7580 - val_loss: 5.3217\n",
      "Epoch 1038/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7817 - val_loss: 5.2540\n",
      "Epoch 1039/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.7722 - val_loss: 5.4267\n",
      "Epoch 1040/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7881 - val_loss: 5.2590\n",
      "Epoch 1041/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7742 - val_loss: 5.3183\n",
      "Epoch 1042/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7591 - val_loss: 5.1674\n",
      "Epoch 1043/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.8008 - val_loss: 6.0335\n",
      "Epoch 1044/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7676 - val_loss: 5.1874\n",
      "Epoch 1045/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7694 - val_loss: 5.1823\n",
      "Epoch 1046/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7847 - val_loss: 5.4793\n",
      "Epoch 1047/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7723 - val_loss: 5.4019\n",
      "Epoch 1048/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8002 - val_loss: 5.3078\n",
      "Epoch 1049/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7733 - val_loss: 5.2683\n",
      "Epoch 1050/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7718 - val_loss: 5.2086\n",
      "Epoch 1051/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7893 - val_loss: 5.2363\n",
      "Epoch 1052/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7890 - val_loss: 5.3397\n",
      "Epoch 1053/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7739 - val_loss: 5.3748\n",
      "Epoch 1054/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7854 - val_loss: 5.4576\n",
      "Epoch 1055/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7877 - val_loss: 5.2504\n",
      "Epoch 1056/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7720 - val_loss: 5.2064\n",
      "Epoch 1057/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7918 - val_loss: 5.3599\n",
      "Epoch 1058/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7920 - val_loss: 5.2723\n",
      "Epoch 1059/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.8371 - val_loss: 5.1849\n",
      "Epoch 1060/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7685 - val_loss: 5.2924\n",
      "Epoch 1061/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7652 - val_loss: 5.2937\n",
      "Epoch 1062/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7656 - val_loss: 5.2402\n",
      "Epoch 1063/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7675 - val_loss: 5.4095\n",
      "Epoch 1064/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7678 - val_loss: 5.2142\n",
      "Epoch 1065/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7774 - val_loss: 5.3619\n",
      "Epoch 1066/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7655 - val_loss: 5.3386\n",
      "Epoch 1067/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8021 - val_loss: 5.2450\n",
      "Epoch 1068/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7772 - val_loss: 5.3080\n",
      "Epoch 1069/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7725 - val_loss: 5.1962\n",
      "Epoch 1070/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8004 - val_loss: 5.3240\n",
      "Epoch 1071/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7755 - val_loss: 5.1437\n",
      "Epoch 1072/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7827 - val_loss: 5.4665\n",
      "Epoch 1073/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7685 - val_loss: 5.2541\n",
      "Epoch 1074/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7819 - val_loss: 5.2308\n",
      "Epoch 1075/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7666 - val_loss: 5.3187\n",
      "Epoch 1076/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7739 - val_loss: 5.2952\n",
      "Epoch 1077/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7975 - val_loss: 5.2716\n",
      "Epoch 1078/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7629 - val_loss: 5.2250\n",
      "Epoch 1079/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7774 - val_loss: 5.3046\n",
      "Epoch 1080/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7634 - val_loss: 5.1889\n",
      "Epoch 1081/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7745 - val_loss: 5.2511\n",
      "Epoch 1082/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7664 - val_loss: 5.6163\n",
      "Epoch 1083/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.8172 - val_loss: 5.2213\n",
      "Epoch 1084/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7599 - val_loss: 5.2387\n",
      "Epoch 1085/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7602 - val_loss: 5.2314\n",
      "Epoch 1086/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7757 - val_loss: 5.3616\n",
      "Epoch 1087/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.7674 - val_loss: 5.2304\n",
      "Epoch 1088/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.8030 - val_loss: 5.2443\n",
      "Epoch 1089/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7725 - val_loss: 5.1383\n",
      "Epoch 1090/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7694 - val_loss: 5.3513\n",
      "Epoch 1091/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7882 - val_loss: 5.2019\n",
      "Epoch 1092/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7726 - val_loss: 5.3112\n",
      "Epoch 1093/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7638 - val_loss: 5.3203\n",
      "Epoch 1094/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7899 - val_loss: 5.5918\n",
      "Epoch 1095/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7715 - val_loss: 5.6461\n",
      "Epoch 1096/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7604 - val_loss: 5.2015\n",
      "Epoch 1097/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7627 - val_loss: 5.2301\n",
      "Epoch 1098/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7503 - val_loss: 5.2435\n",
      "Epoch 1099/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7752 - val_loss: 5.3958\n",
      "Epoch 1100/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7742 - val_loss: 5.2917\n",
      "Epoch 1101/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7839 - val_loss: 5.1603\n",
      "Epoch 1102/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7823 - val_loss: 5.3455\n",
      "Epoch 1103/10000\n",
      "707/707 [==============================] - 1s 965us/step - loss: 4.7697 - val_loss: 5.2649\n",
      "Epoch 1104/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7826 - val_loss: 5.3520\n",
      "Epoch 1105/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7759 - val_loss: 5.4618\n",
      "Epoch 1106/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7728 - val_loss: 5.2304\n",
      "Epoch 1107/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7712 - val_loss: 5.1588\n",
      "Epoch 1108/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7803 - val_loss: 5.2155\n",
      "Epoch 1109/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7680 - val_loss: 5.2409\n",
      "Epoch 1110/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7877 - val_loss: 5.2751\n",
      "Epoch 1111/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.8024 - val_loss: 5.1973\n",
      "Epoch 1112/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7749 - val_loss: 5.1945\n",
      "Epoch 1113/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7759 - val_loss: 5.3890\n",
      "Epoch 1114/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7693 - val_loss: 5.2334\n",
      "Epoch 1115/10000\n",
      "707/707 [==============================] - 1s 957us/step - loss: 4.7558 - val_loss: 5.2014\n",
      "Epoch 1116/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7898 - val_loss: 5.3317\n",
      "Epoch 1117/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7647 - val_loss: 5.4091\n",
      "Epoch 1118/10000\n",
      "707/707 [==============================] - 1s 1000us/step - loss: 4.7778 - val_loss: 5.3788\n",
      "Epoch 1119/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7880 - val_loss: 5.3784\n",
      "Epoch 1120/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7716 - val_loss: 5.3464\n",
      "Epoch 1121/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7773 - val_loss: 5.2331\n",
      "Epoch 1122/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7679 - val_loss: 5.2089\n",
      "Epoch 1123/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7560 - val_loss: 5.2139\n",
      "Epoch 1124/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7688 - val_loss: 5.2027\n",
      "Epoch 1125/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7705 - val_loss: 5.3382\n",
      "Epoch 1126/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7793 - val_loss: 5.2057\n",
      "Epoch 1127/10000\n",
      "707/707 [==============================] - 1s 959us/step - loss: 4.7556 - val_loss: 5.2555\n",
      "Epoch 1128/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7510 - val_loss: 5.2441\n",
      "Epoch 1129/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7844 - val_loss: 5.2151\n",
      "Epoch 1130/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.8044 - val_loss: 5.3545\n",
      "Epoch 1131/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7589 - val_loss: 5.2782\n",
      "Epoch 1132/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7958 - val_loss: 5.2834\n",
      "Epoch 1133/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7501 - val_loss: 5.1945\n",
      "Epoch 1134/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7623 - val_loss: 5.3746\n",
      "Epoch 1135/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7445 - val_loss: 5.2668\n",
      "Epoch 1136/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7769 - val_loss: 5.3735\n",
      "Epoch 1137/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7561 - val_loss: 5.2796\n",
      "Epoch 1138/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7800 - val_loss: 5.2564\n",
      "Epoch 1139/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7706 - val_loss: 5.4470\n",
      "Epoch 1140/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7510 - val_loss: 5.2935\n",
      "Epoch 1141/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.8472 - val_loss: 5.1482\n",
      "Epoch 1142/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7739 - val_loss: 5.1772\n",
      "Epoch 1143/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.7554 - val_loss: 5.2927\n",
      "Epoch 1144/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7673 - val_loss: 5.4630\n",
      "Epoch 1145/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7469 - val_loss: 5.2451\n",
      "Epoch 1146/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7731 - val_loss: 5.3421\n",
      "Epoch 1147/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7563 - val_loss: 5.2274\n",
      "Epoch 1148/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.7552 - val_loss: 5.2625\n",
      "Epoch 1149/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.7533 - val_loss: 5.2583\n",
      "Epoch 1150/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7573 - val_loss: 5.7204\n",
      "Epoch 1151/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7542 - val_loss: 5.3763\n",
      "Epoch 1152/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7454 - val_loss: 5.3198\n",
      "Epoch 1153/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7720 - val_loss: 5.1518\n",
      "Epoch 1154/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7531 - val_loss: 5.3547\n",
      "Epoch 1155/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7826 - val_loss: 5.4204\n",
      "Epoch 1156/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7523 - val_loss: 5.2614\n",
      "Epoch 1157/10000\n",
      "707/707 [==============================] - 1s 967us/step - loss: 4.7658 - val_loss: 5.2053\n",
      "Epoch 1158/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.7668 - val_loss: 5.2266\n",
      "Epoch 1159/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7751 - val_loss: 5.3061\n",
      "Epoch 1160/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7499 - val_loss: 5.4274\n",
      "Epoch 1161/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7656 - val_loss: 5.2524\n",
      "Epoch 1162/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7569 - val_loss: 5.1897\n",
      "Epoch 1163/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7688 - val_loss: 5.2814\n",
      "Epoch 1164/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7479 - val_loss: 5.2293\n",
      "Epoch 1165/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7429 - val_loss: 5.1892\n",
      "Epoch 1166/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7332 - val_loss: 5.3257\n",
      "Epoch 1167/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7579 - val_loss: 5.2494\n",
      "Epoch 1168/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7500 - val_loss: 5.2613\n",
      "Epoch 1169/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7539 - val_loss: 5.2747\n",
      "Epoch 1170/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7661 - val_loss: 5.2217\n",
      "Epoch 1171/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7754 - val_loss: 5.1781\n",
      "Epoch 1172/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7738 - val_loss: 5.1946\n",
      "Epoch 1173/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7430 - val_loss: 5.2033\n",
      "Epoch 1174/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7499 - val_loss: 5.2518\n",
      "Epoch 1175/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7448 - val_loss: 5.2993\n",
      "Epoch 1176/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7659 - val_loss: 5.2185\n",
      "Epoch 1177/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7582 - val_loss: 5.2758\n",
      "Epoch 1178/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.7457 - val_loss: 5.1760\n",
      "Epoch 1179/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.7567 - val_loss: 5.2013\n",
      "Epoch 1180/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7518 - val_loss: 5.2002\n",
      "Epoch 1181/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7575 - val_loss: 5.2863\n",
      "Epoch 1182/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7521 - val_loss: 5.1827\n",
      "Epoch 1183/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7573 - val_loss: 5.1688\n",
      "Epoch 1184/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7617 - val_loss: 5.2413\n",
      "Epoch 1185/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7337 - val_loss: 5.2511\n",
      "Epoch 1186/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7425 - val_loss: 5.3273\n",
      "Epoch 1187/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7692 - val_loss: 5.2130\n",
      "Epoch 1188/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7521 - val_loss: 6.3186\n",
      "Epoch 1189/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7820 - val_loss: 5.3326\n",
      "Epoch 1190/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7658 - val_loss: 5.4758\n",
      "Epoch 1191/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7545 - val_loss: 5.2378\n",
      "Epoch 1192/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7380 - val_loss: 5.2086\n",
      "Epoch 1193/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7561 - val_loss: 5.2005\n",
      "Epoch 1194/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7342 - val_loss: 5.3161\n",
      "Epoch 1195/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7683 - val_loss: 5.1944\n",
      "Epoch 1196/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7631 - val_loss: 5.2916\n",
      "Epoch 1197/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7455 - val_loss: 5.2644\n",
      "Epoch 1198/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7419 - val_loss: 5.2233\n",
      "Epoch 1199/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7608 - val_loss: 5.2014\n",
      "Epoch 1200/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7749 - val_loss: 5.2161\n",
      "Epoch 1201/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7621 - val_loss: 5.2997\n",
      "Epoch 1202/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.7388 - val_loss: 5.3326\n",
      "Epoch 1203/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7479 - val_loss: 5.3167\n",
      "Epoch 1204/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7460 - val_loss: 5.4134\n",
      "Epoch 1205/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7720 - val_loss: 5.2148\n",
      "Epoch 1206/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7608 - val_loss: 5.3074\n",
      "Epoch 1207/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7742 - val_loss: 5.2898\n",
      "Epoch 1208/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7445 - val_loss: 5.2585\n",
      "Epoch 1209/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7508 - val_loss: 5.1816\n",
      "Epoch 1210/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7643 - val_loss: 5.2224\n",
      "Epoch 1211/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7531 - val_loss: 5.2500\n",
      "Epoch 1212/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7503 - val_loss: 5.1509\n",
      "Epoch 1213/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7453 - val_loss: 5.2137\n",
      "Epoch 1214/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7257 - val_loss: 5.2556\n",
      "Epoch 1215/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7562 - val_loss: 5.5715\n",
      "Epoch 1216/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7434 - val_loss: 5.2620\n",
      "Epoch 1217/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7522 - val_loss: 5.3547\n",
      "Epoch 1218/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.7509 - val_loss: 5.3688\n",
      "Epoch 1219/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7345 - val_loss: 5.4767\n",
      "Epoch 1220/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7489 - val_loss: 5.2490\n",
      "Epoch 1221/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7284 - val_loss: 5.2526\n",
      "Epoch 1222/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7755 - val_loss: 5.2348\n",
      "Epoch 1223/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7319 - val_loss: 5.3446\n",
      "Epoch 1224/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7505 - val_loss: 5.2739\n",
      "Epoch 1225/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7604 - val_loss: 5.3248\n",
      "Epoch 1226/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7495 - val_loss: 5.2929\n",
      "Epoch 1227/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7775 - val_loss: 5.2524\n",
      "Epoch 1228/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7555 - val_loss: 5.3603\n",
      "Epoch 1229/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7239 - val_loss: 5.2900\n",
      "Epoch 1230/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7313 - val_loss: 5.4081\n",
      "Epoch 1231/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7678 - val_loss: 5.3026\n",
      "Epoch 1232/10000\n",
      "707/707 [==============================] - 1s 956us/step - loss: 4.7437 - val_loss: 5.2800\n",
      "Epoch 1233/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7510 - val_loss: 5.3519\n",
      "Epoch 1234/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7447 - val_loss: 5.3317\n",
      "Epoch 1235/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7565 - val_loss: 5.2003\n",
      "Epoch 1236/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7415 - val_loss: 5.3014\n",
      "Epoch 1237/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7561 - val_loss: 5.3048\n",
      "Epoch 1238/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7296 - val_loss: 5.3177\n",
      "Epoch 1239/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7404 - val_loss: 5.2338\n",
      "Epoch 1240/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7340 - val_loss: 5.5157\n",
      "Epoch 1241/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7667 - val_loss: 5.3956\n",
      "Epoch 1242/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7411 - val_loss: 5.4450\n",
      "Epoch 1243/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.7523 - val_loss: 5.3996\n",
      "Epoch 1244/10000\n",
      "707/707 [==============================] - 1s 957us/step - loss: 4.7615 - val_loss: 5.2803\n",
      "Epoch 1245/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7346 - val_loss: 5.2336\n",
      "Epoch 1246/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7482 - val_loss: 5.2237\n",
      "Epoch 1247/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7457 - val_loss: 5.2681\n",
      "Epoch 1248/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7489 - val_loss: 5.3194\n",
      "Epoch 1249/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7706 - val_loss: 5.2475\n",
      "Epoch 1250/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7333 - val_loss: 5.2946\n",
      "Epoch 1251/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7691 - val_loss: 5.2677\n",
      "Epoch 1252/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7736 - val_loss: 5.3235\n",
      "Epoch 1253/10000\n",
      "707/707 [==============================] - 1s 953us/step - loss: 4.7561 - val_loss: 5.3879\n",
      "Epoch 1254/10000\n",
      "707/707 [==============================] - 1s 960us/step - loss: 4.7284 - val_loss: 5.2510\n",
      "Epoch 1255/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7621 - val_loss: 5.2765\n",
      "Epoch 1256/10000\n",
      "707/707 [==============================] - 1s 951us/step - loss: 4.7608 - val_loss: 5.2567\n",
      "Epoch 1257/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7640 - val_loss: 5.2569\n",
      "Epoch 1258/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7503 - val_loss: 5.2243\n",
      "Epoch 1259/10000\n",
      "707/707 [==============================] - 1s 965us/step - loss: 4.7478 - val_loss: 5.3667\n",
      "Epoch 1260/10000\n",
      "707/707 [==============================] - 1s 964us/step - loss: 4.7202 - val_loss: 5.2666\n",
      "Epoch 1261/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7424 - val_loss: 5.3336\n",
      "Epoch 1262/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7505 - val_loss: 5.2501\n",
      "Epoch 1263/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7459 - val_loss: 5.2589\n",
      "Epoch 1264/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7439 - val_loss: 5.4192\n",
      "Epoch 1265/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7332 - val_loss: 5.3278\n",
      "Epoch 1266/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7875 - val_loss: 5.2437\n",
      "Epoch 1267/10000\n",
      "707/707 [==============================] - 1s 957us/step - loss: 4.7310 - val_loss: 5.3970\n",
      "Epoch 1268/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7313 - val_loss: 5.2688\n",
      "Epoch 1269/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7451 - val_loss: 5.2365\n",
      "Epoch 1270/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7558 - val_loss: 5.2760\n",
      "Epoch 1271/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7576 - val_loss: 5.1791\n",
      "Epoch 1272/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7306 - val_loss: 5.2193\n",
      "Epoch 1273/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7431 - val_loss: 5.2667\n",
      "Epoch 1274/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7809 - val_loss: 5.2159\n",
      "Epoch 1275/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.7247 - val_loss: 5.3413\n",
      "Epoch 1276/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.7199 - val_loss: 5.2397\n",
      "Epoch 1277/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7614 - val_loss: 5.2995\n",
      "Epoch 1278/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7702 - val_loss: 5.4523\n",
      "Epoch 1279/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7416 - val_loss: 5.3642\n",
      "Epoch 1280/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7587 - val_loss: 5.2226\n",
      "Epoch 1281/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7350 - val_loss: 5.3048\n",
      "Epoch 1282/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7524 - val_loss: 5.2979\n",
      "Epoch 1283/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7337 - val_loss: 5.2810\n",
      "Epoch 1284/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7273 - val_loss: 5.2898\n",
      "Epoch 1285/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7647 - val_loss: 5.2635\n",
      "Epoch 1286/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7494 - val_loss: 5.2574\n",
      "Epoch 1287/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7224 - val_loss: 5.1870\n",
      "Epoch 1288/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7489 - val_loss: 5.3283\n",
      "Epoch 1289/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7415 - val_loss: 5.3194\n",
      "Epoch 1290/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7579 - val_loss: 5.3778\n",
      "Epoch 1291/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7599 - val_loss: 5.3319\n",
      "Epoch 1292/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7303 - val_loss: 5.4049\n",
      "Epoch 1293/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7543 - val_loss: 5.4302\n",
      "Epoch 1294/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7511 - val_loss: 5.3067\n",
      "Epoch 1295/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7422 - val_loss: 5.3259\n",
      "Epoch 1296/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7057 - val_loss: 5.2505\n",
      "Epoch 1297/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7156 - val_loss: 5.3310\n",
      "Epoch 1298/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7122 - val_loss: 5.2738\n",
      "Epoch 1299/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7320 - val_loss: 5.3646\n",
      "Epoch 1300/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7706 - val_loss: 5.6604\n",
      "Epoch 1301/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7369 - val_loss: 5.4754\n",
      "Epoch 1302/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7298 - val_loss: 5.3049\n",
      "Epoch 1303/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7400 - val_loss: 5.2580\n",
      "Epoch 1304/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7358 - val_loss: 5.2339\n",
      "Epoch 1305/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7340 - val_loss: 5.5308\n",
      "Epoch 1306/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7281 - val_loss: 5.3162\n",
      "Epoch 1307/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7415 - val_loss: 5.3974\n",
      "Epoch 1308/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7187 - val_loss: 5.3238\n",
      "Epoch 1309/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7479 - val_loss: 5.2852\n",
      "Epoch 1310/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7211 - val_loss: 5.2962\n",
      "Epoch 1311/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7320 - val_loss: 5.3263\n",
      "Epoch 1312/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7212 - val_loss: 5.2215\n",
      "Epoch 1313/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.7324 - val_loss: 5.3040\n",
      "Epoch 1314/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7736 - val_loss: 5.3621\n",
      "Epoch 1315/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7541 - val_loss: 5.2444\n",
      "Epoch 1316/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7430 - val_loss: 5.2551\n",
      "Epoch 1317/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7468 - val_loss: 5.1853\n",
      "Epoch 1318/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.7358 - val_loss: 5.2782\n",
      "Epoch 1319/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7263 - val_loss: 5.2644\n",
      "Epoch 1320/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7375 - val_loss: 5.3259\n",
      "Epoch 1321/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7492 - val_loss: 5.3508\n",
      "Epoch 1322/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7364 - val_loss: 5.3215\n",
      "Epoch 1323/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7297 - val_loss: 5.3274\n",
      "Epoch 1324/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7211 - val_loss: 5.4448\n",
      "Epoch 1325/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7400 - val_loss: 5.3705\n",
      "Epoch 1326/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7224 - val_loss: 5.4388\n",
      "Epoch 1327/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7267 - val_loss: 5.2430\n",
      "Epoch 1328/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7290 - val_loss: 5.3263\n",
      "Epoch 1329/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7142 - val_loss: 5.3292\n",
      "Epoch 1330/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7346 - val_loss: 5.4993\n",
      "Epoch 1331/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7257 - val_loss: 5.4389\n",
      "Epoch 1332/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7583 - val_loss: 5.2673\n",
      "Epoch 1333/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7374 - val_loss: 5.2628\n",
      "Epoch 1334/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7255 - val_loss: 5.3643\n",
      "Epoch 1335/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7246 - val_loss: 5.2291\n",
      "Epoch 1336/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7207 - val_loss: 5.2640\n",
      "Epoch 1337/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7434 - val_loss: 5.3924\n",
      "Epoch 1338/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7713 - val_loss: 5.2461\n",
      "Epoch 1339/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7199 - val_loss: 5.3015\n",
      "Epoch 1340/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7342 - val_loss: 5.3382\n",
      "Epoch 1341/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7347 - val_loss: 5.4079\n",
      "Epoch 1342/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7259 - val_loss: 5.5421\n",
      "Epoch 1343/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7401 - val_loss: 5.2723\n",
      "Epoch 1344/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7173 - val_loss: 5.3697\n",
      "Epoch 1345/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7490 - val_loss: 5.3561\n",
      "Epoch 1346/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7441 - val_loss: 5.3020\n",
      "Epoch 1347/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7519 - val_loss: 5.2145\n",
      "Epoch 1348/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.7246 - val_loss: 5.3119\n",
      "Epoch 1349/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7342 - val_loss: 5.3863\n",
      "Epoch 1350/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7214 - val_loss: 5.3925\n",
      "Epoch 1351/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7599 - val_loss: 5.3312\n",
      "Epoch 1352/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7139 - val_loss: 5.2733\n",
      "Epoch 1353/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7381 - val_loss: 5.3390\n",
      "Epoch 1354/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7088 - val_loss: 5.3268\n",
      "Epoch 1355/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7457 - val_loss: 5.2515\n",
      "Epoch 1356/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7291 - val_loss: 5.3040\n",
      "Epoch 1357/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7357 - val_loss: 5.3024\n",
      "Epoch 1358/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7376 - val_loss: 5.4601\n",
      "Epoch 1359/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7305 - val_loss: 5.2652\n",
      "Epoch 1360/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7278 - val_loss: 5.3795\n",
      "Epoch 1361/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7631 - val_loss: 5.3365\n",
      "Epoch 1362/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7287 - val_loss: 5.2864\n",
      "Epoch 1363/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7221 - val_loss: 5.3905\n",
      "Epoch 1364/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7235 - val_loss: 5.2753\n",
      "Epoch 1365/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7287 - val_loss: 5.3858\n",
      "Epoch 1366/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7361 - val_loss: 5.5906\n",
      "Epoch 1367/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7321 - val_loss: 5.2975\n",
      "Epoch 1368/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7304 - val_loss: 5.2516\n",
      "Epoch 1369/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7342 - val_loss: 5.3055\n",
      "Epoch 1370/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7248 - val_loss: 5.3516\n",
      "Epoch 1371/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7168 - val_loss: 5.3434\n",
      "Epoch 1372/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7283 - val_loss: 5.3119\n",
      "Epoch 1373/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7464 - val_loss: 5.2464\n",
      "Epoch 1374/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7244 - val_loss: 5.4130\n",
      "Epoch 1375/10000\n",
      "707/707 [==============================] - 1s 985us/step - loss: 4.7181 - val_loss: 5.3842\n",
      "Epoch 1376/10000\n",
      "707/707 [==============================] - 1s 960us/step - loss: 4.7229 - val_loss: 5.3211\n",
      "Epoch 1377/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7290 - val_loss: 5.3534\n",
      "Epoch 1378/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.7246 - val_loss: 5.2555\n",
      "Epoch 1379/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7507 - val_loss: 5.3289\n",
      "Epoch 1380/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7324 - val_loss: 5.3434\n",
      "Epoch 1381/10000\n",
      "707/707 [==============================] - 1s 964us/step - loss: 4.7133 - val_loss: 5.2062\n",
      "Epoch 1382/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7176 - val_loss: 5.3184\n",
      "Epoch 1383/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7315 - val_loss: 5.1997\n",
      "Epoch 1384/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7212 - val_loss: 5.2704\n",
      "Epoch 1385/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7189 - val_loss: 5.2598\n",
      "Epoch 1386/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7026 - val_loss: 5.3900\n",
      "Epoch 1387/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7162 - val_loss: 5.2835\n",
      "Epoch 1388/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7317 - val_loss: 5.3516\n",
      "Epoch 1389/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7369 - val_loss: 5.2975\n",
      "Epoch 1390/10000\n",
      "707/707 [==============================] - 1s 995us/step - loss: 4.7262 - val_loss: 5.2862\n",
      "Epoch 1391/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7157 - val_loss: 5.3727\n",
      "Epoch 1392/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.7398 - val_loss: 5.3929\n",
      "Epoch 1393/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7271 - val_loss: 5.3445\n",
      "Epoch 1394/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7181 - val_loss: 5.3561\n",
      "Epoch 1395/10000\n",
      "707/707 [==============================] - 1s 996us/step - loss: 4.7124 - val_loss: 5.3608\n",
      "Epoch 1396/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7312 - val_loss: 5.4562\n",
      "Epoch 1397/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7081 - val_loss: 5.6150\n",
      "Epoch 1398/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7231 - val_loss: 5.2831\n",
      "Epoch 1399/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7229 - val_loss: 5.3831\n",
      "Epoch 1400/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7377 - val_loss: 5.2410\n",
      "Epoch 1401/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7294 - val_loss: 5.3382\n",
      "Epoch 1402/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7116 - val_loss: 5.3615\n",
      "Epoch 1403/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7247 - val_loss: 5.2948\n",
      "Epoch 1404/10000\n",
      "707/707 [==============================] - 1s 959us/step - loss: 4.7390 - val_loss: 5.2572\n",
      "Epoch 1405/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7319 - val_loss: 5.2782\n",
      "Epoch 1406/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7152 - val_loss: 5.3439\n",
      "Epoch 1407/10000\n",
      "707/707 [==============================] - 1s 983us/step - loss: 4.7316 - val_loss: 5.4073\n",
      "Epoch 1408/10000\n",
      "707/707 [==============================] - 1s 961us/step - loss: 4.7162 - val_loss: 5.4848\n",
      "Epoch 1409/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7345 - val_loss: 5.3626\n",
      "Epoch 1410/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7173 - val_loss: 5.3677\n",
      "Epoch 1411/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7416 - val_loss: 5.2882\n",
      "Epoch 1412/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7259 - val_loss: 5.3459\n",
      "Epoch 1413/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.7084 - val_loss: 5.3315\n",
      "Epoch 1414/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7030 - val_loss: 5.5693\n",
      "Epoch 1415/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7170 - val_loss: 5.2300\n",
      "Epoch 1416/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7104 - val_loss: 5.4323\n",
      "Epoch 1417/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7103 - val_loss: 5.3268\n",
      "Epoch 1418/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7184 - val_loss: 5.3704\n",
      "Epoch 1419/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7171 - val_loss: 5.2669\n",
      "Epoch 1420/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.7294 - val_loss: 5.2549\n",
      "Epoch 1421/10000\n",
      "707/707 [==============================] - 1s 978us/step - loss: 4.7323 - val_loss: 5.2027\n",
      "Epoch 1422/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7274 - val_loss: 5.3511\n",
      "Epoch 1423/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7430 - val_loss: 5.2857\n",
      "Epoch 1424/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7356 - val_loss: 5.4157\n",
      "Epoch 1425/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7240 - val_loss: 5.3214\n",
      "Epoch 1426/10000\n",
      "707/707 [==============================] - 1s 967us/step - loss: 4.7720 - val_loss: 5.2085\n",
      "Epoch 1427/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7248 - val_loss: 5.3963\n",
      "Epoch 1428/10000\n",
      "707/707 [==============================] - 1s 965us/step - loss: 4.7177 - val_loss: 5.3234\n",
      "Epoch 1429/10000\n",
      "707/707 [==============================] - 1s 960us/step - loss: 4.7185 - val_loss: 5.3182\n",
      "Epoch 1430/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7253 - val_loss: 5.3432\n",
      "Epoch 1431/10000\n",
      "707/707 [==============================] - 1s 965us/step - loss: 4.7209 - val_loss: 5.5059\n",
      "Epoch 1432/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7067 - val_loss: 5.2369\n",
      "Epoch 1433/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7014 - val_loss: 5.3396\n",
      "Epoch 1434/10000\n",
      "707/707 [==============================] - 1s 964us/step - loss: 4.7358 - val_loss: 5.3384\n",
      "Epoch 1435/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7364 - val_loss: 5.2925\n",
      "Epoch 1436/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7069 - val_loss: 5.3398\n",
      "Epoch 1437/10000\n",
      "707/707 [==============================] - 1s 990us/step - loss: 4.7408 - val_loss: 5.2668\n",
      "Epoch 1438/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7063 - val_loss: 5.3004\n",
      "Epoch 1439/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7132 - val_loss: 5.3122\n",
      "Epoch 1440/10000\n",
      "707/707 [==============================] - 1s 987us/step - loss: 4.7208 - val_loss: 5.4315\n",
      "Epoch 1441/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7039 - val_loss: 5.3646\n",
      "Epoch 1442/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7012 - val_loss: 5.3138\n",
      "Epoch 1443/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7142 - val_loss: 5.3062\n",
      "Epoch 1444/10000\n",
      "707/707 [==============================] - 1s 999us/step - loss: 4.7113 - val_loss: 5.3006\n",
      "Epoch 1445/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7129 - val_loss: 5.2621\n",
      "Epoch 1446/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7161 - val_loss: 5.2808\n",
      "Epoch 1447/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7350 - val_loss: 5.2908\n",
      "Epoch 1448/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7082 - val_loss: 5.4676\n",
      "Epoch 1449/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7584 - val_loss: 5.3848\n",
      "Epoch 1450/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7475 - val_loss: 5.4036\n",
      "Epoch 1451/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7204 - val_loss: 5.3312\n",
      "Epoch 1452/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7368 - val_loss: 5.2676\n",
      "Epoch 1453/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7057 - val_loss: 5.3595\n",
      "Epoch 1454/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7188 - val_loss: 5.2786\n",
      "Epoch 1455/10000\n",
      "707/707 [==============================] - 1s 989us/step - loss: 4.7430 - val_loss: 5.2223\n",
      "Epoch 1456/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7121 - val_loss: 5.3727\n",
      "Epoch 1457/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7203 - val_loss: 5.3234\n",
      "Epoch 1458/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7072 - val_loss: 5.3622\n",
      "Epoch 1459/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7027 - val_loss: 5.3465\n",
      "Epoch 1460/10000\n",
      "707/707 [==============================] - 1s 998us/step - loss: 4.6990 - val_loss: 5.5003\n",
      "Epoch 1461/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7090 - val_loss: 5.3763\n",
      "Epoch 1462/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7243 - val_loss: 5.5137\n",
      "Epoch 1463/10000\n",
      "707/707 [==============================] - 1s 979us/step - loss: 4.7245 - val_loss: 5.3117\n",
      "Epoch 1464/10000\n",
      "707/707 [==============================] - 1s 965us/step - loss: 4.7253 - val_loss: 5.2925\n",
      "Epoch 1465/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.6910 - val_loss: 5.3356\n",
      "Epoch 1466/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7411 - val_loss: 5.3327\n",
      "Epoch 1467/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.7140 - val_loss: 5.2564\n",
      "Epoch 1468/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.7032 - val_loss: 5.3028\n",
      "Epoch 1469/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7344 - val_loss: 5.2719\n",
      "Epoch 1470/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7113 - val_loss: 5.3084\n",
      "Epoch 1471/10000\n",
      "707/707 [==============================] - 1s 976us/step - loss: 4.7105 - val_loss: 5.3810\n",
      "Epoch 1472/10000\n",
      "707/707 [==============================] - 1s 991us/step - loss: 4.7233 - val_loss: 5.4253\n",
      "Epoch 1473/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7086 - val_loss: 5.5187\n",
      "Epoch 1474/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7171 - val_loss: 5.3064\n",
      "Epoch 1475/10000\n",
      "707/707 [==============================] - 1s 997us/step - loss: 4.7124 - val_loss: 5.3519\n",
      "Epoch 1476/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7218 - val_loss: 5.2905\n",
      "Epoch 1477/10000\n",
      "707/707 [==============================] - 1s 994us/step - loss: 4.7087 - val_loss: 5.4119\n",
      "Epoch 1478/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.6937 - val_loss: 5.3465\n",
      "Epoch 1479/10000\n",
      "707/707 [==============================] - 1s 986us/step - loss: 4.7716 - val_loss: 5.2873\n",
      "Epoch 1480/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7031 - val_loss: 5.2300\n",
      "Epoch 1481/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7061 - val_loss: 5.3246\n",
      "Epoch 1482/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.6976 - val_loss: 5.2594\n",
      "Epoch 1483/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7139 - val_loss: 5.2441\n",
      "Epoch 1484/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7169 - val_loss: 5.3396\n",
      "Epoch 1485/10000\n",
      "707/707 [==============================] - 1s 968us/step - loss: 4.7238 - val_loss: 5.4901\n",
      "Epoch 1486/10000\n",
      "707/707 [==============================] - 1s 969us/step - loss: 4.6898 - val_loss: 5.3795\n",
      "Epoch 1487/10000\n",
      "707/707 [==============================] - 1s 984us/step - loss: 4.6976 - val_loss: 5.5639\n",
      "Epoch 1488/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7356 - val_loss: 5.2502\n",
      "Epoch 1489/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7112 - val_loss: 5.3661\n",
      "Epoch 1490/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.6927 - val_loss: 5.3492\n",
      "Epoch 1491/10000\n",
      "707/707 [==============================] - 1s 980us/step - loss: 4.7080 - val_loss: 5.2903\n",
      "Epoch 1492/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7043 - val_loss: 5.2881\n",
      "Epoch 1493/10000\n",
      "707/707 [==============================] - 1s 975us/step - loss: 4.7100 - val_loss: 5.2583\n",
      "Epoch 1494/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.6942 - val_loss: 5.2532\n",
      "Epoch 1495/10000\n",
      "707/707 [==============================] - 1s 977us/step - loss: 4.6988 - val_loss: 5.2605\n",
      "Epoch 1496/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.7243 - val_loss: 5.3330\n",
      "Epoch 1497/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.6959 - val_loss: 5.3910\n",
      "Epoch 1498/10000\n",
      "707/707 [==============================] - 1s 960us/step - loss: 4.7234 - val_loss: 5.3836\n",
      "Epoch 1499/10000\n",
      "707/707 [==============================] - 1s 972us/step - loss: 4.7159 - val_loss: 5.2243\n",
      "Epoch 1500/10000\n",
      "707/707 [==============================] - 1s 966us/step - loss: 4.7046 - val_loss: 5.3520\n",
      "Epoch 1501/10000\n",
      "707/707 [==============================] - 1s 992us/step - loss: 4.6986 - val_loss: 5.2347\n",
      "Epoch 1502/10000\n",
      "707/707 [==============================] - 1s 971us/step - loss: 4.7007 - val_loss: 5.3107\n",
      "Epoch 1503/10000\n",
      "707/707 [==============================] - 1s 982us/step - loss: 4.6987 - val_loss: 5.3633\n",
      "Epoch 1504/10000\n",
      "707/707 [==============================] - 1s 981us/step - loss: 4.6954 - val_loss: 5.2166\n",
      "Epoch 1505/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.6986 - val_loss: 5.3007\n",
      "Epoch 1506/10000\n",
      "707/707 [==============================] - 1s 988us/step - loss: 4.7133 - val_loss: 5.3990\n",
      "Epoch 1507/10000\n",
      "707/707 [==============================] - 1s 973us/step - loss: 4.7151 - val_loss: 5.3085\n",
      "Epoch 1508/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7050 - val_loss: 5.3345\n",
      "Epoch 1509/10000\n",
      "707/707 [==============================] - 1s 970us/step - loss: 4.7146 - val_loss: 5.3477\n",
      "Epoch 1510/10000\n",
      "707/707 [==============================] - 1s 993us/step - loss: 4.6908 - val_loss: 5.2895\n",
      "Epoch 1511/10000\n",
      "707/707 [==============================] - 1s 974us/step - loss: 4.6873 - val_loss: 5.3260\n",
      "Epoch 1512/10000\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.6877 - val_loss: 5.3088\n",
      "Epoch 1513/10000\n",
      "690/707 [============================>.] - ETA: 0s - loss: 4.7035Restoring model weights from the end of the best epoch: 513.\n",
      "707/707 [==============================] - 1s 1ms/step - loss: 4.7049 - val_loss: 5.4110\n",
      "Epoch 1513: early stopping\n",
      "197/197 [==============================] - 0s 692us/step\n",
      "Mean squared error: 5.25\n",
      "Coefficient of determination: 0.20\n",
      "Mean absolute error: 1.82\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# optimizer = RMSprop(learning_rate=learning_rate)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1000, verbose=1, restore_best_weights=True)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64 , activation='relu'))\n",
    "model.add(Dense(16 , activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# Fit while printing accuracy\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=10000, batch_size=32,\n",
    "          verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Mean squared error: %.2f' % mse)\n",
    "print('Coefficient of determination: %.2f' % r2)\n",
    "print('Mean absolute error: %.2f' % mae_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaEnv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
